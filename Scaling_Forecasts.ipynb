{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecfa55c0-4c02-46e6-ad73-52ba26a368e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to open file...\n",
      "❌ ERROR: The file is STILL locked. Try restarting your laptop or moving the file to your Desktop.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "\n",
    "input_path = r\"C:\\scaling_forecasts.csv\"\n",
    "output_path = r'C:\\Users\\noahi\\Downloads\\scaling_forecasts_CLEANED.csv'\n",
    "\n",
    "# This clears any lingering Python connections to the file\n",
    "gc.collect()\n",
    "\n",
    "print(\"Attempting to open file...\")\n",
    "\n",
    "try:\n",
    "    # We use 'with' which ensures the file is closed immediately after reading\n",
    "    with pd.read_csv(input_path, chunksize=10000, encoding='latin-1', on_bad_lines='skip') as reader:\n",
    "        # Delete the old output if it exists to avoid conflicts\n",
    "        if os.path.exists(output_path):\n",
    "            os.remove(output_path)\n",
    "            \n",
    "        for chunk in reader:\n",
    "            # Clean\n",
    "            chunk = chunk.dropna(how='all').drop_duplicates()\n",
    "            \n",
    "            # Save\n",
    "            chunk.to_csv(output_path, mode='a', index=False, header=not os.path.exists(output_path))\n",
    "            \n",
    "    print(\"✅ FINAL SUCCESS! The cleaned file is ready.\")\n",
    "\n",
    "except PermissionError:\n",
    "    print(\"❌ ERROR: The file is STILL locked. Try restarting your laptop or moving the file to your Desktop.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ DIFFERENT ERROR: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3d0f061-41d3-42fd-8575-1afcb70cd75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made a temp copy to bypass the lock...\n",
      "✅ SUCCESS! Cleaned file created.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "original_path = r\"C:\\scaling_forecasts.csv\\scaling_forecasts.csv\"\n",
    "\n",
    "temp_path = r'C:\\Users\\noahi\\Downloads\\temp_copy.csv'\n",
    "output_path = r'C:\\Users\\noahi\\Downloads\\scaling_forecasts_CLEANED.csv'\n",
    "\n",
    "try:\n",
    "    # 1. Force a copy of the file\n",
    "    shutil.copy2(original_path, temp_path)\n",
    "    print(\"Made a temp copy to bypass the lock...\")\n",
    "\n",
    "    # 2. Process the TEMP copy\n",
    "    with pd.read_csv(temp_path, chunksize=10000, encoding='latin-1', on_bad_lines='skip') as reader:\n",
    "        if os.path.exists(output_path):\n",
    "            os.remove(output_path)\n",
    "            \n",
    "        for chunk in reader:\n",
    "            chunk = chunk.dropna(how='all').drop_duplicates()\n",
    "            chunk.to_csv(output_path, mode='a', index=False, header=not os.path.exists(output_path))\n",
    "            \n",
    "    print(\"✅ SUCCESS! Cleaned file created.\")\n",
    "    \n",
    "    # 3. Cleanup temp file\n",
    "    os.remove(temp_path)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "277244e2-c130-4e4a-a5c9-802615d870bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Preview ---\n",
      "   campus_id            timestamp  projected_deliveries  required_robots  \\\n",
      "0        3.0  2023-06-21 07:28:00                 108.0              7.0   \n",
      "1        7.0  2025-03-21 04:13:00                 112.0              6.0   \n",
      "2        5.0  2024-06-27 10:59:00                 134.0             11.0   \n",
      "3        9.0  2020-07-29 06:57:00                 116.0             16.0   \n",
      "4        3.0  2022-06-09 12:15:00                  87.0             17.0   \n",
      "\n",
      "   expansion_cost  \n",
      "0     3110.191200  \n",
      "1     8393.164235  \n",
      "2     9426.772879  \n",
      "3     2183.157559  \n",
      "4     3581.815198  \n",
      "\n",
      "Total rows in cleaned file: 835071\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load just the first 5 rows to peek at the data\n",
    "preview = pd.read_csv(r'C:\\Users\\noahi\\Downloads\\scaling_forecasts_CLEANED.csv', nrows=5)\n",
    "\n",
    "print(\"--- Data Preview ---\")\n",
    "print(preview)\n",
    "\n",
    "# Note: Counting rows in a huge file can be slow, \n",
    "# but this is the memory-efficient way to do it:\n",
    "row_count = 0\n",
    "for chunk in pd.read_csv(r'C:\\Users\\noahi\\Downloads\\scaling_forecasts_CLEANED.csv', chunksize=50000):\n",
    "    row_count += len(chunk)\n",
    "\n",
    "print(f\"\\nTotal rows in cleaned file: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6851c0c-3939-44ea-9bd6-1ae15f319513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global clean finished. Final row count: 835071\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r'C:\\Users\\noahi\\Downloads\\scaling_forecasts_CLEANED.csv'\n",
    "\n",
    "# Since the blanks are gone, the file is smaller. \n",
    "# Let's try to load just the important columns to see if we can \n",
    "# do a final global duplicate check.\n",
    "df = pd.read_csv(path, low_memory=False)\n",
    "\n",
    "# Final global deduplicate\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Save the absolute final version\n",
    "df.to_csv(r'C:\\Users\\noahi\\Downloads\\scaling_forecasts_FINAL.csv', index=False)\n",
    "\n",
    "print(f\"Global clean finished. Final row count: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5bd69bb-5155-4cab-84f2-0f89231e704b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "campus_id               float64\n",
      "timestamp                object\n",
      "projected_deliveries    float64\n",
      "required_robots         float64\n",
      "expansion_cost          float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\noahi\\Downloads\\scaling_forecasts_FINAL.csv', nrows=1)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "512b783a-ca51-4d0c-991e-6639c1a5ce5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'to_currency'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mcampus_id\u001b[39m\u001b[33m'\u001b[39m] = pd.to_numeric(df[\u001b[33m'\u001b[39m\u001b[33mcampus_id\u001b[39m\u001b[33m'\u001b[39m], errors=\u001b[33m'\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Convert a 'Date' column to actual dates\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mexpansion_cost\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_currency\u001b[49m (df[\u001b[33m'\u001b[39m\u001b[33mexpansion_cost\u001b[39m\u001b[33m'\u001b[39m], errors=\u001b[33m'\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Save one last time with correct types\u001b[39;00m\n\u001b[32m     10\u001b[39m df.to_csv(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mnoahi\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDownloads\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mscaling_forecasts_FINAL_TYPED.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'pandas' has no attribute 'to_currency'"
     ]
    }
   ],
   "source": [
    "# Convert a 'Forecast_Value' column to actual numbers (float)\n",
    "# errors='coerce' turns any remaining text junk into 'NaN' so it doesn't crash\n",
    "df['campus_id'] = pd.to_numeric(df['campus_id'], errors='coerce')\n",
    "\n",
    "# Convert a 'Date' column to actual dates\n",
    "\n",
    "df['expansion_cost'] = pd.to_currency (df['expansion_cost'], errors='coerce')\n",
    "\n",
    "# Save one last time with correct types\n",
    "df.to_csv(r'C:\\Users\\noahi\\Downloads\\scaling_forecasts_FINAL_TYPED.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ddde811-d5bd-4f9f-841b-240649069991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\$'\n",
      "C:\\Users\\noahi\\AppData\\Local\\Temp\\ipykernel_10748\\821589754.py:12: SyntaxWarning: invalid escape sequence '\\$'\n",
      "  df['expansion_cost'] = df['expansion_cost'].replace('[\\$,]', '', regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success! Data types converted and file saved.\n",
      "campus_id               float64\n",
      "timestamp                object\n",
      "projected_deliveries    float64\n",
      "required_robots         float64\n",
      "expansion_cost          float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load the file\n",
    "path = r'C:\\Users\\noahi\\Downloads\\scaling_forecasts_FINAL.csv'\n",
    "df = pd.read_csv(path, low_memory=False)\n",
    "\n",
    "# 2. Fix campus_id (Numeric)\n",
    "df['campus_id'] = pd.to_numeric(df['campus_id'], errors='coerce')\n",
    "\n",
    "# 3. Fix expansion_cost (Currency)\n",
    "# We remove '$' and ',' then convert to float\n",
    "df['expansion_cost'] = df['expansion_cost'].replace('[\\$,]', '', regex=True)\n",
    "df['expansion_cost'] = pd.to_numeric(df['expansion_cost'], errors='coerce')\n",
    "\n",
    "# 4. Fix Date (Update 'date_column' to your actual column name, e.g., 'date')\n",
    "# If your column is named 'date', use:\n",
    "if 'date' in df.columns:\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "elif 'forecast_date' in df.columns:\n",
    "    df['forecast_date'] = pd.to_datetime(df['forecast_date'], errors='coerce')\n",
    "\n",
    "# 5. Save the result\n",
    "output_path = r'C:\\Users\\noahi\\Downloads\\scaling_forecasts_FINAL_TYPED.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"✅ Success! Data types converted and file saved.\")\n",
    "print(df.dtypes) # This will show you the new types (float64, datetime, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ddd252e-1322-43d0-8cdf-3a915cf7fd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       expansion_cost      campus_id\n",
      "count    8.341840e+05  834143.000000\n",
      "mean              NaN       4.417082\n",
      "std               NaN      33.175158\n",
      "min              -inf    -999.000000\n",
      "25%      3.242957e+03       3.000000\n",
      "50%      5.499307e+03       6.000000\n",
      "75%      7.753601e+03       8.000000\n",
      "max               inf      10.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noahi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\_methods.py:52: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n"
     ]
    }
   ],
   "source": [
    "# This shows you the Min, Max, and Average for your numbers\n",
    "print(df[['expansion_cost', 'campus_id']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d394b52-4204-485f-a265-4f94d44f0188",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['project_deliveries'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# This shows you the Min, Max, and Average for your numbers\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mexpansion_cost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcampus_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproject_deliveries\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrequired_robots\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m.describe())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4107\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4108\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4110\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6249\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6252\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['project_deliveries'] not in index\""
     ]
    }
   ],
   "source": [
    "# This shows you the Min, Max, and Average for your numbers\n",
    "print(df[['expansion_cost', 'campus_id', 'project_deliveries', 'required_robots']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18bb2783-155a-422e-9fc9-6f47d5d5b8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expansion_cost\n",
      " inf              167\n",
      "-inf              163\n",
      "-9.999997e+08       8\n",
      "-9.999997e+295      6\n",
      "-9.999997e+07       5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# This shows you the top 5 most frequent values in a column\n",
    "# Helpful for seeing if \"N/A\" or \"0\" is filling up your data\n",
    "print(df['expansion_cost'].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93cf74c3-6477-4794-954a-fc332b3e599f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cleaned Column Stats ---\n",
      "count     8.335580e+05\n",
      "mean     1.332974e+302\n",
      "std                inf\n",
      "min       1.000016e+03\n",
      "25%       3.245973e+03\n",
      "50%       5.500809e+03\n",
      "75%       7.753513e+03\n",
      "max      9.999997e+307\n",
      "Name: expansion_cost, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noahi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\nanops.py:1016: RuntimeWarning: overflow encountered in square\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Replace Infinity with NaN\n",
    "df['expansion_cost'] = df['expansion_cost'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 2. Replace those specific weird negative placeholders\n",
    "# We catch anything smaller than a logical minimum (e.g., negative costs)\n",
    "df.loc[df['expansion_cost'] < -1000, 'expansion_cost'] = np.nan\n",
    "\n",
    "# 3. Final Check: See if they are gone\n",
    "print(\"--- Cleaned Column Stats ---\")\n",
    "print(df['expansion_cost'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d4548cd-bb0d-4a11-a3ff-42b22dbe548b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Trailing spaces removed from all text columns!\n"
     ]
    }
   ],
   "source": [
    "# This goes through every text column and removes extra spaces at the start or end\n",
    "df_obj = df.select_dtypes(['object'])\n",
    "df[df_obj.columns] = df_obj.apply(lambda x: x.str.strip())\n",
    "\n",
    "print(\"✅ Trailing spaces removed from all text columns!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b29d8c01-aa6c-4c51-bca4-5c8f5afea022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New Data Types ---\n",
      "campus_id                      float64\n",
      "timestamp               datetime64[ns]\n",
      "projected_deliveries           float64\n",
      "required_robots                float64\n",
      "expansion_cost                 float64\n",
      "dtype: object\n",
      "\n",
      "--- First 5 Rows ---\n",
      "   campus_id           timestamp  projected_deliveries  required_robots  \\\n",
      "0        3.0 2023-06-21 07:28:00                 108.0              7.0   \n",
      "1        7.0 2025-03-21 04:13:00                 112.0              6.0   \n",
      "2        5.0 2024-06-27 10:59:00                 134.0             11.0   \n",
      "3        9.0 2020-07-29 06:57:00                 116.0             16.0   \n",
      "4        3.0 2022-06-09 12:15:00                  87.0             17.0   \n",
      "\n",
      "   expansion_cost  \n",
      "0     3110.191200  \n",
      "1     8393.164235  \n",
      "2     9426.772879  \n",
      "3     2183.157559  \n",
      "4     3581.815198  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Convert Timestamp to actual Date/Time objects\n",
    "# This allows you to filter by month, day, or hour later\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "\n",
    "# 2. Ensure numbers are floats (Decimals)\n",
    "# We already cleaned expansion_cost, but we'll ensure the others are set too\n",
    "num_cols = ['projected_deliveries', 'required_robots', 'campus_id', 'expansion_cost']\n",
    "\n",
    "for col in num_cols:\n",
    "    # Convert to numeric and turn errors/text into NaN\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# 3. Final Sanity Check: Remove any rows that became completely empty after conversion\n",
    "df = df.dropna(how='all')\n",
    "\n",
    "print(\"--- New Data Types ---\")\n",
    "print(df.dtypes)\n",
    "print(\"\\n--- First 5 Rows ---\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f6c0e9-bf0f-40a0-b66a-68f99d94bfe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0f64df6-3b61-4b5e-a2cb-7473b21c5d13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed expansion_cost: Replaced outliers with median: 5,499.29\n",
      "Fixed projected_deliveries: Replaced outliers with median: 100.00\n",
      "Fixed required_robots: Replaced outliers with median: 10.00\n",
      "\n",
      "--- Final Cleaned Statistics ---\n",
      "       expansion_cost  projected_deliveries  required_robots\n",
      "count    8.350710e+05          8.350710e+05     8.350710e+05\n",
      "mean     6.828947e+03          2.614505e+02    -3.763708e+01\n",
      "std      1.099814e+06          1.643765e+05     2.969883e+05\n",
      "min      1.000016e+03         -1.480000e+07    -1.900000e+08\n",
      "25%      3.250057e+03          9.300000e+01     5.000000e+00\n",
      "50%      5.499289e+03          1.000000e+02     1.000000e+01\n",
      "75%      7.746154e+03          1.070000e+02     1.500000e+01\n",
      "max      9.999997e+08          1.480000e+08     1.900000e+08\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load your latest file\n",
    "path = r'C:\\Users\\noahi\\Downloads\\scaling_forecasts_PRODUCTION_READY.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# 2. Define the columns to fix\n",
    "cols_to_fix = ['expansion_cost', 'projected_deliveries', 'required_robots']\n",
    "\n",
    "for col in cols_to_fix:\n",
    "    # A. First, convert 'inf' and extreme placeholders to NaN so they don't \n",
    "    # ruin the median calculation.\n",
    "    # We'll treat anything larger than 1 billion as an error/placeholder.\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df.loc[df[col].abs() > 1e9, col] = np.nan\n",
    "    \n",
    "    # B. Calculate the Median (the middle value) of the REAL data\n",
    "    median_value = df[col].median()\n",
    "    \n",
    "    # C. Fill the NaNs (the former outliers) with that median\n",
    "    df[col] = df[col].fillna(median_value)\n",
    "    \n",
    "    print(f\"Fixed {col}: Replaced outliers with median: {median_value:,.2f}\")\n",
    "\n",
    "# 3. Final Check: Your Mean and Std should now look normal\n",
    "print(\"\\n--- Final Cleaned Statistics ---\")\n",
    "print(df[cols_to_fix].describe())\n",
    "\n",
    "# 4. Save the Final, Final version\n",
    "df.to_csv(r'C:\\Users\\noahi\\Downloads\\scaling_forecasts_FINAL_CLEAN.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0c6c9ac-4ae2-4812-a6f4-9d3133c4012a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = r'C:\\Users\\noahi\\Downloads\\scaling_forecasts_PRODUCTION_READY.csv'\n",
    "print(os.path.exists(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a476451-1dee-4cd2-bba4-cabef03fa88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved and ready for reporting!\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(r'C:\\Users\\noahi\\Downloads\\scaling_forecasts_PRODUCTION_READY.csv', index=False)\n",
    "print(\"File saved and ready for reporting!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90e0f512-caf2-4151-ba9e-acd9b07db264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed expansion_cost: Replaced outliers with median: 5,499.29\n",
      "Fixed projected_deliveries: Replaced outliers with median: 100.00\n",
      "Fixed required_robots: Replaced outliers with median: 10.00\n",
      "\n",
      "--- Final Cleaned Statistics ---\n",
      "       expansion_cost  projected_deliveries  required_robots\n",
      "count    8.350710e+05          8.350710e+05     8.350710e+05\n",
      "mean     6.828947e+03          2.614505e+02    -3.763708e+01\n",
      "std      1.099814e+06          1.643765e+05     2.969883e+05\n",
      "min      1.000016e+03         -1.480000e+07    -1.900000e+08\n",
      "25%      3.250057e+03          9.300000e+01     5.000000e+00\n",
      "50%      5.499289e+03          1.000000e+02     1.000000e+01\n",
      "75%      7.746154e+03          1.070000e+02     1.500000e+01\n",
      "max      9.999997e+08          1.480000e+08     1.900000e+08\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load your latest file\n",
    "path = r'C:\\Users\\noahi\\Downloads\\scaling_forecasts_PRODUCTION_READY.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# 2. Define the columns to fix\n",
    "cols_to_fix = ['expansion_cost', 'projected_deliveries', 'required_robots']\n",
    "\n",
    "for col in cols_to_fix:\n",
    "    # A. First, convert 'inf' and extreme placeholders to NaN so they don't \n",
    "    # ruin the median calculation.\n",
    "    # We'll treat anything larger than 1 billion as an error/placeholder.\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df.loc[df[col].abs() > 1e9, col] = np.nan\n",
    "    \n",
    "    # B. Calculate the Median (the middle value) of the REAL data\n",
    "    median_value = df[col].median()\n",
    "    \n",
    "    # C. Fill the NaNs (the former outliers) with that median\n",
    "    df[col] = df[col].fillna(median_value)\n",
    "    \n",
    "    print(f\"Fixed {col}: Replaced outliers with median: {median_value:,.2f}\")\n",
    "\n",
    "# 3. Final Check: Your Mean and Std should now look normal\n",
    "print(\"\\n--- Final Cleaned Statistics ---\")\n",
    "print(df[cols_to_fix].describe())\n",
    "\n",
    "# 4. Save the Final, Final version\n",
    "df.to_csv(r'C:\\Users\\noahi\\Downloads\\scaling_forecasts_FINAL_CLEAN.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f115c393-33c3-433d-8e79-fd4b068022d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data is now 'Tidy': No duplicates, no blanks in key areas, and math is safe.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Global Clean: Blanks & Duplicates\n",
    "df = df.dropna(how='all') # Only drops if the ENTIRE row is empty\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# 2. Numeric Clean: Median for Outliers (Math columns only)\n",
    "math_cols = ['expansion_cost', 'projected_deliveries', 'required_robots']\n",
    "\n",
    "for col in math_cols:\n",
    "    # Remove the \"Infinity\" and \"Ghost\" values first\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df.loc[df[col].abs() > 1e9, col] = np.nan\n",
    "    \n",
    "    # Fill gaps and outliers with Median\n",
    "    med = df[col].median()\n",
    "    df[col] = df[col].fillna(med)\n",
    "\n",
    "# 3. ID & Date Clean: Remove rows with missing IDs (they are useless)\n",
    "df = df.dropna(subset=['campus_id', 'timestamp'])\n",
    "\n",
    "print(\"✅ Data is now 'Tidy': No duplicates, no blanks in key areas, and math is safe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3fd7e6-d2ef-408f-806a-f0d77b012f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "which file bath file is the last tidy data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
