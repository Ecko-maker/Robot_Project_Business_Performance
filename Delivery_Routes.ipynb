{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55ca3ed7-655d-4ee2-9bf3-5a2bb2530a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File detected!\n",
      "--- Columns Found ---\n",
      "['delivery_id', 'robot_id', 'start_time', 'start_lat', 'start_lon', 'end_lat', 'end_lon', 'distance', 'duration', 'obstacles_encountered', 'traffic_level', 'end_time']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Remember to use the 'r' before the quotes for the file path!\n",
    "file_path = r\"C:\\Users\\noahi\\Downloads\\delivery_routes.csv\\delivery_routes.csv\"\n",
    "try:\n",
    "    # We only load the first 5 rows to save your RAM\n",
    "    df_preview = pd.read_csv(file_path, nrows=5)\n",
    "    print(\"✅ File detected!\")\n",
    "    print(f\"--- Columns Found ---\")\n",
    "    print(df_preview.columns.tolist())\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be5847a2-63a9-4de1-8d64-d073db6c00c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CAMPUS LOGISTICS: CLEANING AUDIT ---\n",
      "Original Rows:    737,904\n",
      "Cleaned Rows:     726,888\n",
      "Noise Removed:    11,016\n",
      "Status:           PROFESSIONAL GRADE ✅\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 1. Setup Paths\n",
    "input_path = r\"C:\\Users\\noahi\\Downloads\\demand_forecasts.csv\\demand_forecasts.csv\" # Update if name differs\n",
    "output_path = r\"C:\\Users\\noahi\\Downloads\\demand_forecasts_PROFESSIONAL.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(input_path)\n",
    "    raw_rows = len(df)\n",
    "\n",
    "    # 2. DEDUPLICATION\n",
    "    # Removing rows where the same campus, time, and day are repeated\n",
    "    df = df.drop_duplicates(subset=['timestamp', 'campus_id'])\n",
    "\n",
    "    # 3. REMOVE BLANKS\n",
    "    # Drop rows missing the primary order_count or campus_id\n",
    "    df = df.dropna(subset=['order_count', 'campus_id'])\n",
    "\n",
    "    # 4. MEDIAN OUTLIER REPLACEMENT\n",
    "    # Target: order_count (to fix sensor glitches or input errors)\n",
    "    col = 'order_count'\n",
    "    col_median = df[col].median()\n",
    "    \n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Replace impossible order counts with the median\n",
    "    df.loc[(df[col] < lower_bound) | (df[col] > upper_bound), col] = col_median\n",
    "\n",
    "    # 5. DATA OPTIMIZATION (Laptop Protection)\n",
    "    # Downcasting numeric columns to save RAM\n",
    "    if 'campus_id' in df.columns:\n",
    "        df['campus_id'] = pd.to_numeric(df['campus_id'], downcast='integer')\n",
    "    df['order_count'] = pd.to_numeric(df['order_count'], downcast='float')\n",
    "\n",
    "    # 6. FINAL EXPORT\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(\"--- CAMPUS LOGISTICS: CLEANING AUDIT ---\")\n",
    "    print(f\"Original Rows:    {raw_rows:,}\")\n",
    "    print(f\"Cleaned Rows:     {len(df):,}\")\n",
    "    print(f\"Noise Removed:    {raw_rows - len(df):,}\")\n",
    "    print(f\"Status:           PROFESSIONAL GRADE ✅\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during cleanup: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f66c7fc4-29c7-468f-8d5e-e94ef9d6e81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DELIVERY ROUTES: CLEANING AUDIT ---\n",
      "Original Rows:    737,904\n",
      "Cleaned Rows:     735,271\n",
      "Glitches Removed: 2,633\n",
      "Status:           PROFESSIONAL GRADE ✅\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 1. Setup Paths\n",
    "input_path = r\"C:\\Users\\noahi\\Downloads\\delivery_routes.csv\\delivery_routes.csv\"\n",
    "output_path = r\"C:\\Users\\noahi\\Downloads\\delivery_routes_PROFESSIONAL.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(input_path)\n",
    "    raw_rows = len(df)\n",
    "\n",
    "    # 2. DEDUPLICATION\n",
    "    # Each delivery should be unique\n",
    "    df = df.drop_duplicates(subset=['delivery_id'])\n",
    "\n",
    "    # 3. REMOVE BLANKS\n",
    "    # Drop rows where critical tracking data is missing\n",
    "    df = df.dropna(subset=['delivery_id', 'robot_id', 'distance', 'duration'])\n",
    "\n",
    "    # 4. SURGICAL OUTLIER FIX (Numeric Only)\n",
    "    # This identifies columns with numbers and ignores text like \"medium\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        # We don't want to change IDs even if they look like outliers\n",
    "        if 'id' in col.lower():\n",
    "            continue\n",
    "            \n",
    "        col_median = df[col].median()\n",
    "        \n",
    "        # IQR Calculation\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Replace sensor glitches with the median\n",
    "        df.loc[(df[col] < lower_bound) | (df[col] > upper_bound), col] = col_median\n",
    "\n",
    "    # 5. LAPTOP PROTECTION (Memory Optimization)\n",
    "    # Downcast floats and ints to smaller types to save RAM\n",
    "    df['distance'] = pd.to_numeric(df['distance'], downcast='float')\n",
    "    df['duration'] = pd.to_numeric(df['duration'], downcast='float')\n",
    "\n",
    "    # 6. FINAL EXPORT\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(\"--- DELIVERY ROUTES: CLEANING AUDIT ---\")\n",
    "    print(f\"Original Rows:    {raw_rows:,}\")\n",
    "    print(f\"Cleaned Rows:     {len(df):,}\")\n",
    "    print(f\"Glitches Removed: {raw_rows - len(df):,}\")\n",
    "    print(f\"Status:           PROFESSIONAL GRADE ✅\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during cleanup: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45b1d5c5-bc73-467d-bda4-01676b022ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DELIVERY ROUTES: SIZE COMPARISON ---\n",
      "Original File: 131.01 MB\n",
      "Cleaned File:  118.62 MB\n",
      "Difference:    -12.39 MB\n",
      "\n",
      "Note: Size decreased because we removed thousands of duplicate or 'noisy' rows.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Paths for the Delivery Routes files\n",
    "raw_path = r\"C:\\Users\\noahi\\Downloads\\delivery_routes.csv\\delivery_routes.csv\"\n",
    "clean_path = r\"C:\\Users\\noahi\\Downloads\\delivery_routes.csv\\delivery_routes_PROFESSIONAL.csv\"\n",
    "\n",
    "def get_mb(path):\n",
    "    return os.path.getsize(path) / (1024 * 1024)\n",
    "\n",
    "if os.path.exists(raw_path) and os.path.exists(clean_path):\n",
    "    raw_size = get_mb(raw_path)\n",
    "    clean_size = get_mb(clean_path)\n",
    "    \n",
    "    print(\"--- DELIVERY ROUTES: SIZE COMPARISON ---\")\n",
    "    print(f\"Original File: {raw_size:.2f} MB\")\n",
    "    print(f\"Cleaned File:  {clean_size:.2f} MB\")\n",
    "    print(f\"Difference:    {clean_size - raw_size:+.2f} MB\")\n",
    "    \n",
    "    if clean_size > raw_size:\n",
    "        print(\"\\nNote: Size increased slightly because we filled missing data gaps with Median values.\")\n",
    "    else:\n",
    "        print(\"\\nNote: Size decreased because we removed thousands of duplicate or 'noisy' rows.\")\n",
    "else:\n",
    "    print(\"❌ One of the files was not found. Please check the file names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027c52f5-9c69-4628-86bd-bb842bc78038",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
