{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f589f5c-02f5-4947-a29d-c8e842c660be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Step 1 Complete: Blanks and local duplicates removed without using RAM.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_path = r\"C:\\Users\\noahi\\Downloads\\delivery_reliability.csv\\delivery_reliability.csv\"\n",
    "output_path = r'C:\\Users\\noahi\\Downloads\\scaling_forecasts_STEP1.csv'\n",
    "\n",
    "# We process 50,000 rows at a time\n",
    "chunk_size = 50000\n",
    "is_first_chunk = True\n",
    "\n",
    "for chunk in pd.read_csv(input_path, chunksize=chunk_size, low_memory=False):\n",
    "    # 1. Immediate Clean: Remove completely empty rows\n",
    "    chunk = chunk.dropna(how='all')\n",
    "    \n",
    "    # 2. Immediate Clean: Remove duplicates found WITHIN this batch\n",
    "    chunk = chunk.drop_duplicates()\n",
    "    \n",
    "    # 3. Write directly to disk (Append mode 'a')\n",
    "    chunk.to_csv(output_path, mode='a', index=False, header=is_first_chunk)\n",
    "    \n",
    "    is_first_chunk = False\n",
    "\n",
    "print(\"âœ… Step 1 Complete: Blanks and local duplicates removed without using RAM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e008930-93ff-4603-8a2f-65dc92b3e142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Your Dataset Columns ---\n",
      "['delivery_id', 'timestamp', 'success', 'delay_minutes', 'customer_satisfaction']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Update this path to your new file location\n",
    "path = r\"C:\\Users\\noahi\\Downloads\\delivery_reliability.csv\\delivery_reliability.csv\"\n",
    "\n",
    "# Read only the header (row 0)\n",
    "header = pd.read_csv(path, nrows=0)\n",
    "\n",
    "print(\"--- Your Dataset Columns ---\")\n",
    "print(header.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12341986-342d-4719-8e6e-46810b0998ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Medians found: {'delay_minutes': np.float64(7.49095903536962), 'customer_satisfaction': np.float64(3.0)}\n",
      "ðŸš€ Step 2 Complete: Final cleaned file created!\n"
     ]
    }
   ],
   "source": [
    "# A. Get Medians from our new Step 1 file\n",
    "math_cols = ['delay_minutes', 'customer_satisfaction']\n",
    "temp_df = pd.read_csv(output_path, usecols=math_cols)\n",
    "\n",
    "medians = {}\n",
    "for col in math_cols:\n",
    "    # Clean the column to find a real median\n",
    "    s = pd.to_numeric(temp_df[col], errors='coerce')\n",
    "    s = s.replace([float('inf'), float('-inf')], pd.NA).dropna()\n",
    "    medians[col] = s.median()\n",
    "\n",
    "print(f\"ðŸ“Š Medians found: {medians}\")\n",
    "\n",
    "# B. Apply fix to final file\n",
    "final_path = r'C:\\Users\\noahi\\Downloads\\delivery_data_FINAL.csv'\n",
    "is_first_chunk = True\n",
    "\n",
    "for chunk in pd.read_csv(output_path, chunksize=50000):\n",
    "    # Fix the Math columns\n",
    "    for col in math_cols:\n",
    "        chunk[col] = pd.to_numeric(chunk[col], errors='coerce')\n",
    "        # Replace extreme outliers (e.g., > 1 week of delay) with median\n",
    "        chunk.loc[chunk[col].abs() > 10080, col] = medians[col] \n",
    "        chunk[col] = chunk[col].fillna(medians[col])\n",
    "    \n",
    "    # Fix the Timestamp\n",
    "    chunk['timestamp'] = pd.to_datetime(chunk['timestamp'], errors='coerce')\n",
    "    \n",
    "    chunk.to_csv(final_path, mode='a', index=False, header=is_first_chunk)\n",
    "    is_first_chunk = False\n",
    "\n",
    "print(\"ðŸš€ Step 2 Complete: Final cleaned file created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6b6e51e-26a1-4755-95f6-73ac40846299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Medians found: {'delay_minutes': np.float64(7.49095903536962), 'customer_satisfaction': np.float64(3.0)}\n",
      "ðŸš€ Step 2 Complete: Final cleaned file created!\n"
     ]
    }
   ],
   "source": [
    "# A. Get Medians from our new Step 1 file\n",
    "math_cols = ['delay_minutes', 'customer_satisfaction']\n",
    "temp_df = pd.read_csv(output_path, usecols=math_cols)\n",
    "\n",
    "medians = {}\n",
    "for col in math_cols:\n",
    "    # Clean the column to find a real median\n",
    "    s = pd.to_numeric(temp_df[col], errors='coerce')\n",
    "    s = s.replace([float('inf'), float('-inf')], pd.NA).dropna()\n",
    "    medians[col] = s.median()\n",
    "\n",
    "print(f\"ðŸ“Š Medians found: {medians}\")\n",
    "\n",
    "# B. Apply fix to final file\n",
    "final_path = r'C:\\Users\\noahi\\Downloads\\delivery_data_FINAL.csv'\n",
    "is_first_chunk = True\n",
    "\n",
    "for chunk in pd.read_csv(output_path, chunksize=50000):\n",
    "    # Fix the Math columns\n",
    "    for col in math_cols:\n",
    "        chunk[col] = pd.to_numeric(chunk[col], errors='coerce')\n",
    "        # Replace extreme outliers (e.g., > 1 week of delay) with median\n",
    "        chunk.loc[chunk[col].abs() > 10080, col] = medians[col] \n",
    "        chunk[col] = chunk[col].fillna(medians[col])\n",
    "    \n",
    "    # Fix the Timestamp\n",
    "    chunk['timestamp'] = pd.to_datetime(chunk['timestamp'], errors='coerce')\n",
    "    \n",
    "    chunk.to_csv(final_path, mode='a', index=False, header=is_first_chunk)\n",
    "    is_first_chunk = False\n",
    "\n",
    "print(\"ðŸš€ Step 2 Complete: Final cleaned file created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ac906cc-ad0c-4551-8c2a-da7e972a5ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw Data Preview ---\n",
      "   delivery_id            timestamp  success  delay_minutes  \\\n",
      "0          1.0  2025-03-20 16:26:00      1.0      11.974611   \n",
      "1          2.0  2022-11-28 02:22:00      1.0       1.193637   \n",
      "2          3.0  2023-10-13 22:00:00      1.0       9.050722   \n",
      "3          4.0  2024-07-23 23:55:00      1.0       2.575706   \n",
      "4          5.0  2025-08-23 12:42:00      1.0       3.078317   \n",
      "\n",
      "   customer_satisfaction  \n",
      "0                    5.0  \n",
      "1                    5.0  \n",
      "2                    1.0  \n",
      "3                    5.0  \n",
      "4                    1.0  \n",
      "\n",
      "--- Quick Math Audit ---\n",
      "       delay_minutes  customer_satisfaction\n",
      "count    1000.000000            1000.000000\n",
      "mean        7.348064               2.971000\n",
      "std         4.305070               1.407885\n",
      "min         0.023085               1.000000\n",
      "25%         3.554658               2.000000\n",
      "50%         7.096370               3.000000\n",
      "75%        10.986690               4.000000\n",
      "max        14.999648               5.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r\"C:\\Users\\noahi\\Downloads\\delivery_data_FINAL.csv\" \n",
    "\n",
    "# We only load 5 rows to peek at the formatting\n",
    "preview = pd.read_csv(path, nrows=5)\n",
    "print(\"--- Raw Data Preview ---\")\n",
    "print(preview)\n",
    "\n",
    "# Check if 'delay_minutes' has any impossible values (like we saw before)\n",
    "# We'll read a slightly larger chunk just for this check\n",
    "check_math = pd.read_csv(path, usecols=['delay_minutes', 'customer_satisfaction'], nrows=1000)\n",
    "print(\"\\n--- Quick Math Audit ---\")\n",
    "print(check_math.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a1442b6-133d-41e0-9783-c98758dc4607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cleaned Data Audit ---\n",
      "Date Range: 2020-01-12 06:21:00 to 2025-09-14 17:58:00\n",
      "\n",
      "--- Cleaned Math (No more Infinity!) ---\n",
      "       delay_minutes  customer_satisfaction\n",
      "count     100.000000             100.000000\n",
      "mean        7.585615               2.970000\n",
      "std         4.183811               1.403135\n",
      "min         0.060910               1.000000\n",
      "25%         3.943290               2.000000\n",
      "50%         7.736929               3.000000\n",
      "75%        10.958187               4.000000\n",
      "max        14.991491               5.000000\n"
     ]
    }
   ],
   "source": [
    "final_path = r\"C:\\Users\\noahi\\Downloads\\delivery_data_FINAL.csv\"\n",
    "\n",
    "# Load a small sample of the CLEANED data\n",
    "cleaned_sample = pd.read_csv(final_path, nrows=100)\n",
    "\n",
    "print(\"--- Cleaned Data Audit ---\")\n",
    "# This proves the timestamp is now recognized as a datetime object\n",
    "cleaned_sample['timestamp'] = pd.to_datetime(cleaned_sample['timestamp'])\n",
    "print(f\"Date Range: {cleaned_sample['timestamp'].min()} to {cleaned_sample['timestamp'].max()}\")\n",
    "\n",
    "# This proves the math is safe\n",
    "print(\"\\n--- Cleaned Math (No more Infinity!) ---\")\n",
    "print(cleaned_sample[['delay_minutes', 'customer_satisfaction']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50350f26-6385-42c4-af71-126e24312d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original File: 44.01 MB\n",
      "Cleaned File: 44.73 MB\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'seen_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOriginal File: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_size\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m MB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCleaned File: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcleaned_size\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m MB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRows removed (approx): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mseen_ids\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'seen_ids' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "original_size = os.path.getsize(input_path) / (1024 * 1024)\n",
    "cleaned_size = os.path.getsize(output_path) / (1024 * 1024)\n",
    "\n",
    "print(f\"Original File: {original_size:.2f} MB\")\n",
    "print(f\"Cleaned File: {cleaned_size:.2f} MB\")\n",
    "print(f\"Rows removed (approx): {len(seen_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1165465-2dd2-4b73-8946-b6ea1dc672cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Rows: 835,071\n",
      "Cleaned Rows:  1,670,143\n",
      "Difference:    -835072 rows removed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Let's count the rows in both files\n",
    "def count_rows(file_path):\n",
    "    count = 0\n",
    "    for chunk in pd.read_csv(file_path, chunksize=100000, usecols=[0]):\n",
    "        count += len(chunk)\n",
    "    return count\n",
    "\n",
    "original_rows = count_rows(r\"C:\\Users\\noahi\\Downloads\\delivery_reliability.csv\\delivery_reliability.csv\")\n",
    "cleaned_rows = count_rows(r'C:\\Users\\noahi\\Downloads\\delivery_data_FINAL.csv')\n",
    "\n",
    "print(f\"Original Rows: {original_rows:,}\")\n",
    "print(f\"Cleaned Rows:  {cleaned_rows:,}\")\n",
    "print(f\"Difference:    {original_rows - cleaned_rows} rows removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0126bc50-7a5d-4b70-bfda-8dde4e8f4426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Cleaned! Final unique row count should be around 833,283\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "input_path = r\"C:\\Users\\noahi\\Downloads\\delivery_reliability.csv\\delivery_reliability.csv\"\n",
    "output_path = r'C:\\Users\\noahi\\Downloads\\delivery_data_STEP1.csv'\n",
    "\n",
    "# --- THE FRESH START ---\n",
    "# This deletes the old \"Double\" file so we start with a blank slate\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)\n",
    "\n",
    "seen_ids = set()\n",
    "is_first_chunk = True\n",
    "\n",
    "for chunk in pd.read_csv(input_path, chunksize=50000, low_memory=False):\n",
    "    # 1. Drop rows where the entire row is blank\n",
    "    chunk = chunk.dropna(how='all')\n",
    "    \n",
    "    # 2. Drop duplicates within this batch based on delivery_id\n",
    "    chunk = chunk.drop_duplicates(subset=['delivery_id'])\n",
    "    \n",
    "    # 3. Filter out IDs we've already processed in previous batches\n",
    "    chunk = chunk[~chunk['delivery_id'].isin(seen_ids)]\n",
    "    \n",
    "    # Update our memory of seen IDs\n",
    "    seen_ids.update(chunk['delivery_id'].tolist())\n",
    "    \n",
    "    # 4. Write to disk\n",
    "    chunk.to_csv(output_path, mode='a', index=False, header=is_first_chunk)\n",
    "    is_first_chunk = False\n",
    "\n",
    "print(f\"âœ… Cleaned! Final unique row count should be around {len(seen_ids):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3e442fa-c647-4c29-a0b2-33350fc6a35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Calculated Medians: {'delay_minutes': np.float64(7.490866960536037), 'customer_satisfaction': np.float64(3.0)}\n",
      "ðŸš€ Master Clean Complete! File saved to: C:\\Users\\noahi\\Downloads\\delivery_data_FINAL.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Path to the file we just deduplicated\n",
    "input_path = r'C:\\Users\\noahi\\Downloads\\delivery_data_STEP1.csv'\n",
    "final_path = r'C:\\Users\\noahi\\Downloads\\delivery_data_FINAL.csv'\n",
    "\n",
    "# 1. CALCULATE THE MEDIANS (The \"Truth\")\n",
    "# We load only the columns we need to save RAM\n",
    "math_cols = ['delay_minutes', 'customer_satisfaction']\n",
    "temp_df = pd.read_csv(input_path, usecols=math_cols)\n",
    "\n",
    "medians = {}\n",
    "for col in math_cols:\n",
    "    # Convert to numeric, turn \"inf\" and text into NaN\n",
    "    clean_col = pd.to_numeric(temp_df[col], errors='coerce')\n",
    "    # Replace Infinity with NaN so they don't spoil the median\n",
    "    clean_col = clean_col.replace([np.inf, -np.inf], np.nan)\n",
    "    medians[col] = clean_col.median()\n",
    "\n",
    "print(f\"âœ… Calculated Medians: {medians}\")\n",
    "\n",
    "# 2. THE FINAL CLEANING PASS\n",
    "is_first_chunk = True\n",
    "\n",
    "# We use chunking again just to be safe on your RAM\n",
    "for chunk in pd.read_csv(input_path, chunksize=100000):\n",
    "    \n",
    "    for col in math_cols:\n",
    "        # Convert the column to numbers\n",
    "        chunk[col] = pd.to_numeric(chunk[col], errors='coerce')\n",
    "        \n",
    "        # LOGIC: If a value is ridiculously high (> 10,000) or Infinity, \n",
    "        # swap it with the Median we calculated.\n",
    "        # 10,000 minutes is about 7 daysâ€”anything higher is likely an error.\n",
    "        limit = 10000 if col == 'delay_minutes' else 10 \n",
    "        \n",
    "        chunk.loc[chunk[col].abs() > limit, col] = medians[col]\n",
    "        chunk.loc[np.isinf(chunk[col]), col] = medians[col]\n",
    "        \n",
    "        # Fill any remaining blanks (NaNs) with the Median too\n",
    "        chunk[col] = chunk[col].fillna(medians[col])\n",
    "        \n",
    "    # 3. FIX THE TIMESTAMP FORMAT\n",
    "    chunk['timestamp'] = pd.to_datetime(chunk['timestamp'], errors='coerce')\n",
    "    \n",
    "    # Save the result\n",
    "    chunk.to_csv(final_path, mode='a' if not is_first_chunk else 'w', index=False)\n",
    "    is_first_chunk = False\n",
    "\n",
    "print(f\"ðŸš€ Master Clean Complete! File saved to: {final_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56b9130b-be35-4f30-a7ea-7f014859af83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            delay_minutes customer_satisfaction\n",
      "count              833291                833291\n",
      "unique             831524                     7\n",
      "top     7.490866960536037                   3.0\n",
      "freq                 1760                168435\n"
     ]
    }
   ],
   "source": [
    "print(pd.read_csv(final_path)[['delay_minutes', 'customer_satisfaction']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e10dd07f-0213-4832-975f-35ab59639491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- THE FINAL AUDIT ---\n",
      "       delay_minutes\n",
      "count  833283.000000\n",
      "mean        7.495393\n",
      "std         4.631865\n",
      "min      -150.000000\n",
      "25%         3.750000\n",
      "50%         7.490000\n",
      "75%        11.230000\n",
      "max      1500.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "final_path = r'C:\\Users\\noahi\\Downloads\\delivery_data_FINAL.csv'\n",
    "\n",
    "# Load the data one last time\n",
    "df = pd.read_csv(final_path)\n",
    "\n",
    "# 1. FORCE Numeric conversion\n",
    "# This turns \"7.49...\" into the actual number 7.49\n",
    "df['delay_minutes'] = pd.to_numeric(df['delay_minutes'], errors='coerce')\n",
    "\n",
    "# 2. Final Rounding (Optional but makes it cleaner)\n",
    "# Rounds the messy decimals to 2 places (e.g., 7.49)\n",
    "df['delay_minutes'] = df['delay_minutes'].round(2)\n",
    "\n",
    "# 3. Save it back\n",
    "df.to_csv(final_path, index=False)\n",
    "\n",
    "# 4. THE REAL TEST\n",
    "print(\"--- THE FINAL AUDIT ---\")\n",
    "print(df[['delay_minutes', 'customer_satisfaction']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63c7f31e-8be4-4d7c-bb44-aef4e27af945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Calculated Sanity Medians: {'expansion_cost': np.float64(5499.300698556359), 'projected_deliveries': np.float64(100.0), 'required_robots': np.float64(10.0)}\n",
      "ðŸš€ Scaling Forecasts are now Clean! Saved to: C:\\Users\\noahi\\Downloads\\scaling_forecasts_MASTER_CLEAN.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "input_path = r\"C:\\Users\\noahi\\Downloads\\scaling_forecasts.csv\\scaling_forecasts.csv\"\n",
    "output_path = r'C:\\Users\\noahi\\Downloads\\scaling_forecasts_MASTER_CLEAN.csv'\n",
    "\n",
    "# --- THE FRESH START ---\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)\n",
    "\n",
    "# 1. CALCULATE THE MEDIANS (One quick pass)\n",
    "# We calculate medians first so we have a 'sane' number to replace Infinity with\n",
    "cols_to_fix = ['expansion_cost', 'projected_deliveries', 'required_robots']\n",
    "temp_df = pd.read_csv(input_path, usecols=cols_to_fix)\n",
    "\n",
    "medians = {}\n",
    "for col in cols_to_fix:\n",
    "    s = pd.to_numeric(temp_df[col], errors='coerce')\n",
    "    s = s.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    medians[col] = s.median()\n",
    "\n",
    "print(f\"ðŸ“Š Calculated Sanity Medians: {medians}\")\n",
    "\n",
    "# 2. THE STREAMING CLEAN\n",
    "is_first_chunk = True\n",
    "for chunk in pd.read_csv(input_path, chunksize=50000):\n",
    "    \n",
    "    # Remove blank rows and duplicates\n",
    "    chunk = chunk.dropna(how='all').drop_duplicates()\n",
    "    \n",
    "    for col in cols_to_fix:\n",
    "        # Force to numeric\n",
    "        chunk[col] = pd.to_numeric(chunk[col], errors='coerce')\n",
    "        \n",
    "        # Replace 'inf' or massive placeholders (> 1 billion) with the median\n",
    "        chunk.loc[chunk[col].abs() > 1e9, col] = medians[col]\n",
    "        chunk.loc[np.isinf(chunk[col]), col] = medians[col]\n",
    "        \n",
    "        # Fill gaps and round to make it professional\n",
    "        chunk[col] = chunk[col].fillna(medians[col])\n",
    "        if col == 'required_robots':\n",
    "            chunk[col] = chunk[col].round(0).astype(int)\n",
    "            \n",
    "    # Write to disk\n",
    "    chunk.to_csv(output_path, mode='a', index=False, header=is_first_chunk)\n",
    "    is_first_chunk = False\n",
    "\n",
    "print(f\"ðŸš€ Scaling Forecasts are now Clean! Saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8e45522-9dc6-421b-82b6-f24a2601449c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Professional Scaling Audit ---\n",
      "       expansion_cost  required_robots\n",
      "count    8.350710e+05     8.350710e+05\n",
      "mean    -3.354726e+03    -3.763708e+01\n",
      "std      3.293863e+06     2.969883e+05\n",
      "min     -9.999997e+08    -1.900000e+08\n",
      "25%      3.249929e+03     5.000000e+00\n",
      "50%      5.499301e+03     1.000000e+01\n",
      "75%      7.746154e+03     1.500000e+01\n",
      "max      9.999997e+08     1.900000e+08\n"
     ]
    }
   ],
   "source": [
    "final_df = pd.read_csv(output_path)\n",
    "print(\"--- Professional Scaling Audit ---\")\n",
    "print(final_df[['expansion_cost', 'required_robots']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "621731b0-916f-4866-ad35-5586d3d177ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- THE FINAL SURGICAL AUDIT ---\n",
      "       expansion_cost  required_robots\n",
      "count   835071.000000    835071.000000\n",
      "mean      5498.435262         9.996639\n",
      "std       2597.313543         5.469420\n",
      "min       1000.015510         1.000000\n",
      "25%       3250.057462         5.000000\n",
      "50%       5499.300699        10.000000\n",
      "75%       7746.122004        15.000000\n",
      "max       9999.996599        19.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "path = r'C:\\Users\\noahi\\Downloads\\scaling_forecasts_MASTER_CLEAN.csv'\n",
    "\n",
    "# 1. Load the data\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "for col in ['expansion_cost', 'required_robots']:\n",
    "    # Define the \"Safe Zone\" based on your 25% and 75% marks\n",
    "    q1 = df[col].quantile(0.25)\n",
    "    q3 = df[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    # Anything beyond this is likely a -999,999,999 placeholder\n",
    "    lower_bound = q1 - (1.5 * iqr)\n",
    "    upper_bound = q3 + (1.5 * iqr)\n",
    "    \n",
    "    # Get the median to use as a replacement\n",
    "    median_val = df[col].median()\n",
    "    \n",
    "    # \"Clip\" the data: Replace anything outside the bounds with the median\n",
    "    df.loc[(df[col] < lower_bound) | (df[col] > upper_bound), col] = median_val\n",
    "\n",
    "# 2. Final Rounding\n",
    "df['required_robots'] = df['required_robots'].round(0).astype(int)\n",
    "\n",
    "# 3. Save as the TRUE Professional version\n",
    "final_path = r'C:\\Users\\noahi\\Downloads\\scaling_forecasts_PROFESSIONAL_FINAL.csv'\n",
    "df.to_csv(final_path, index=False)\n",
    "\n",
    "print(\"--- THE FINAL SURGICAL AUDIT ---\")\n",
    "print(df[['expansion_cost', 'required_robots']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40289b05-bf6c-4f2e-b3cd-3958e67975e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Version                | File Size      \n",
      "--------------------------------------------------\n",
      "Original Raw Data              | 41.61 MB       \n",
      "Professional Final Data        | 40.75 MB       \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "original_path = r\"C:\\Users\\noahi\\Downloads\\scaling_forecasts.csv\\scaling_forecasts.csv\"\n",
    "pro_path = r'C:\\Users\\noahi\\Downloads\\scaling_forecasts_PROFESSIONAL_FINAL.csv'\n",
    "\n",
    "def get_mb(path):\n",
    "    if os.path.exists(path):\n",
    "        return f\"{os.path.getsize(path) / (1024 * 1024):.2f} MB\"\n",
    "    return \"File Not Found\"\n",
    "\n",
    "print(f\"{'Dataset Version':<30} | {'File Size':<15}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Original Raw Data':<30} | {get_mb(original_path):<15}\")\n",
    "print(f\"{'Professional Final Data':<30} | {get_mb(pro_path):<15}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c426cf6-7ae7-42c8-a6da-da74d0fc90b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New Dataset Structure ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   delivery_id    5 non-null      float64\n",
      " 1   timestamp      5 non-null      object \n",
      " 2   revenue        5 non-null      float64\n",
      " 3   cost           5 non-null      float64\n",
      " 4   profit         5 non-null      float64\n",
      " 5   delivery_type  5 non-null      object \n",
      "dtypes: float64(4), object(2)\n",
      "memory usage: 372.0+ bytes\n",
      "None\n",
      "\n",
      "--- First 5 Rows ---\n",
      "   delivery_id            timestamp    revenue       cost     profit  \\\n",
      "0          1.0  2020-06-06 03:47:00  42.240739  10.322332  16.816082   \n",
      "1          2.0  2024-05-26 11:30:00  15.869323  18.341229  15.294924   \n",
      "2          3.0  2021-07-18 19:24:00   5.972007   7.685768   7.629064   \n",
      "3          4.0  2021-10-15 11:56:00  42.108121  19.720245  29.812982   \n",
      "4          5.0  2020-09-14 19:10:00  15.630525   4.195106   4.388067   \n",
      "\n",
      "  delivery_type  \n",
      "0         robot  \n",
      "1         robot  \n",
      "2         robot  \n",
      "3         human  \n",
      "4         robot  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Update this path to the new file you want to clean\n",
    "new_file_path = r\"C:\\Users\\noahi\\Downloads\\delivery_financials.csv\\delivery_financials.csv\" \n",
    "\n",
    "# Let's peek at the first few rows and the data types\n",
    "df_preview = pd.read_csv(new_file_path, nrows=5)\n",
    "\n",
    "print(\"--- New Dataset Structure ---\")\n",
    "print(df_preview.info())\n",
    "print(\"\\n--- First 5 Rows ---\")\n",
    "print(df_preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd6e61b2-b9e5-44c3-aa16-b02319690939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Financial Dataset Cleaned and Re-Calculated!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "input_path = r\"C:\\Users\\noahi\\Downloads\\delivery_financials.csv\\delivery_financials.csv\"\n",
    "output_path = r'C:\\Users\\noahi\\Downloads\\delivery_financial_data_PROFESSIONAL.csv'\n",
    "\n",
    "# 1. Fresh Start\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)\n",
    "\n",
    "seen_ids = set()\n",
    "is_first_chunk = True\n",
    "\n",
    "for chunk in pd.read_csv(input_path, chunksize=100000):\n",
    "    # Remove blanks and duplicate IDs\n",
    "    chunk = chunk.dropna(how='all')\n",
    "    chunk = chunk.drop_duplicates(subset=['delivery_id'])\n",
    "    chunk = chunk[~chunk['delivery_id'].isin(seen_ids)]\n",
    "    seen_ids.update(chunk['delivery_id'].tolist())\n",
    "\n",
    "    # Fix Numeric Columns\n",
    "    for col in ['revenue', 'cost', 'profit']:\n",
    "        chunk[col] = pd.to_numeric(chunk[col], errors='coerce')\n",
    "        \n",
    "        # Use IQR or \"Sanity Limits\" to remove placeholders (like 999 million)\n",
    "        # Assuming most deliveries are between $0 and $1000\n",
    "        median_val = chunk[col].median()\n",
    "        chunk.loc[chunk[col].abs() > 10000, col] = median_val \n",
    "        chunk.loc[np.isinf(chunk[col]), col] = median_val\n",
    "        chunk[col] = chunk[col].fillna(median_val)\n",
    "\n",
    "    # THE CRITICAL STEP: Fix the math\n",
    "    # Professional datasets must have internal consistency\n",
    "    chunk['profit'] = chunk['revenue'] - chunk['cost']\n",
    "\n",
    "    # Fix Dates\n",
    "    chunk['timestamp'] = pd.to_datetime(chunk['timestamp'], errors='coerce')\n",
    "\n",
    "    # Save\n",
    "    chunk.to_csv(output_path, mode='a', index=False, header=is_first_chunk)\n",
    "    is_first_chunk = False\n",
    "\n",
    "print(\"âœ… Financial Dataset Cleaned and Re-Calculated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42fc0bb4-5640-4a90-8851-32c3c71a1294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FINANCIAL INTEGRITY AUDIT ---\n",
      "Math is 100% correct: False\n",
      "\n",
      "--- Profit Overview ---\n",
      "count    833573.000000\n",
      "mean         17.039317\n",
      "std          15.270491\n",
      "min       -1963.945001\n",
      "25%           5.802179\n",
      "50%          17.043727\n",
      "75%          28.274527\n",
      "max        4981.835907\n",
      "Name: profit, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "final_finance = r\"C:\\Users\\noahi\\Downloads\\delivery_financial_data_PROFESSIONAL.csv\"\n",
    "df = pd.read_csv(final_finance)\n",
    "\n",
    "print(\"--- FINANCIAL INTEGRITY AUDIT ---\")\n",
    "# This checks if our manual calculation is correct\n",
    "check = (df['revenue'] - df['cost']) == df['profit']\n",
    "print(f\"Math is 100% correct: {check.all()}\")\n",
    "\n",
    "print(\"\\n--- Profit Overview ---\")\n",
    "print(df['profit'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cace1ea9-1cf8-41d7-9a05-c31b78e59054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- THE FINAL FINANCIAL AUDIT ---\n",
      "Math is 100% correct: True\n",
      "\n",
      "--- Corrected Profit Stats ---\n",
      "count    833573.000000\n",
      "mean         17.035732\n",
      "std          14.103533\n",
      "min        -191.360000\n",
      "25%           5.800000\n",
      "50%          17.040000\n",
      "75%          28.270000\n",
      "max         492.500000\n",
      "Name: profit, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r\"C:\\Users\\noahi\\Downloads\\delivery_financial_data_PROFESSIONAL.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# 1. Force the Math & Round to 2 decimals (Cents)\n",
    "df['revenue'] = df['revenue'].round(2)\n",
    "df['cost'] = df['cost'].round(2)\n",
    "df['profit'] = (df['revenue'] - df['cost']).round(2)\n",
    "\n",
    "# 2. Final Outlier Check (Removing the $4,000 \"Glitch\" deliveries)\n",
    "# Realistically, a delivery shouldn't exceed $500 in revenue or cost\n",
    "for col in ['revenue', 'cost']:\n",
    "    median_val = df[col].median()\n",
    "    df.loc[df[col].abs() > 500, col] = median_val\n",
    "\n",
    "# 3. Recalculate Profit one last time after clipping\n",
    "df['profit'] = (df['revenue'] - df['cost']).round(2)\n",
    "\n",
    "# 4. Save and Verify\n",
    "df.to_csv(path, index=False)\n",
    "\n",
    "print(\"--- THE FINAL FINANCIAL AUDIT ---\")\n",
    "check = (df['revenue'] - df['cost']).round(2) == df['profit']\n",
    "print(f\"Math is 100% correct: {check.all()}\")\n",
    "print(\"\\n--- Corrected Profit Stats ---\")\n",
    "print(df['profit'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0050e77-f458-4d04-9af5-809bd25d665b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Version                | File Size      \n",
      "--------------------------------------------------\n",
      "Original Financial Data        | 71.78 MB       \n",
      "Professional Financial Final   | 41.93 MB       \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "orig_path = r\"C:\\Users\\noahi\\Downloads\\delivery_financials.csv\\delivery_financials.csv\"\n",
    "final_path = r\"C:\\Users\\noahi\\Downloads\\delivery_financial_data_PROFESSIONAL.csv\"\n",
    "\n",
    "def get_mb(path):\n",
    "    if os.path.exists(path):\n",
    "        return f\"{os.path.getsize(path) / (1024 * 1024):.2f} MB\"\n",
    "    return \"File Not Found\"\n",
    "\n",
    "print(f\"{'Dataset Version':<30} | {'File Size':<15}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Original Financial Data':<30} | {get_mb(orig_path):<15}\")\n",
    "print(f\"{'Professional Financial Final':<30} | {get_mb(final_path):<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90fc924-24fa-4deb-963c-a0dbd5a66ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
