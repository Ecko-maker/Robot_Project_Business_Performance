{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31b0a1fd-34e3-4fdb-b0c2-bd9b9c5eb84d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\noahi\\\\Downloads\\\\delivery_cost_comparison\\\\delivery_cost_comparison.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 1. Let's see what is actually in your Downloads\u001b[39;00m\n\u001b[32m      5\u001b[39m path = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mnoahi\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDownloads\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdelivery_cost_comparison\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdelivery_cost_comparison.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m files = [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f.endswith(\u001b[33m'\u001b[39m\u001b[33m.csv\u001b[39m\u001b[33m'\u001b[39m)]\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- Files found in Downloads ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(files):\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\noahi\\\\Downloads\\\\delivery_cost_comparison\\\\delivery_cost_comparison.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Let's see what is actually in your Downloads\n",
    "path = r\"C:\\Users\\noahi\\Downloads\\delivery_cost_comparison\\delivery_cost_comparison.csv\"\n",
    "files = [f for f in os.listdir(path) if f.endswith('.csv')]\n",
    "\n",
    "print(\"--- Files found in Downloads ---\")\n",
    "for i, f in enumerate(files):\n",
    "    print(f\"{i}: {f}\")\n",
    "\n",
    "# 2. Automatically try to grab the most recent one that ISN'T our clean files\n",
    "# (Adjust the index [i] if it picks the wrong one)\n",
    "try:\n",
    "    target_file = os.path.join(path, files[-1]) \n",
    "    print(f\"\\nAttempting to open: {target_file}\")\n",
    "    \n",
    "    df_preview = pd.read_csv(target_file, nrows=5)\n",
    "    print(\"\\n--- Success! Columns found: ---\")\n",
    "    print(df_preview.columns.tolist())\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "363b3279-a603-4bce-a0a9-ab97dc75b024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Still having trouble: [Errno 2] No such file or directory: 'C:\\\\Users\\\\noahi\\\\Downloads\\\\delivery_cost_comparison\\\\delivery_cost_comparison.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# We point directly to the file, not the folder\n",
    "target_file = r\"C:\\Users\\noahi\\Downloads\\delivery_cost_comparison\\delivery_cost_comparison.csv\"\n",
    "\n",
    "try:\n",
    "    # Let's peek at the first 5 rows\n",
    "    df_preview = pd.read_csv(target_file, nrows=5)\n",
    "    print(\"--- Success! Dataset Loaded ---\")\n",
    "    print(\"\\nColumns:\", df_preview.columns.tolist())\n",
    "    print(\"\\nSample Data:\\n\", df_preview.head())\n",
    "    \n",
    "    # Check the total size while we are at it\n",
    "    # This helps us know if we need the 'Streaming Method'\n",
    "    total_rows = pd.read_csv(target_file, usecols=[0]).shape[0]\n",
    "    print(f\"\\nTotal Row Count: {total_rows:,}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Still having trouble: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5828cbed-e8ef-40cd-bd6f-65f0ba963c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Path is still wrong. Let's check the Downloads folder for the real name:\n",
      "Folders found in Downloads: ['.ipynb_checkpoints', 'Application', 'CDIR', 'Certificate & Evalution', 'Communication', 'DA YUU', 'daikibo-telemetry-data.json', 'delivery_cost_comparison.csv', 'delivery_financials.csv', 'delivery_reliability.csv', 'Excel in work place', 'Reflection Report - 1756243885133_files', 'scaling_forecasts.csv', 'SIP', 'Software', 'Tech Essential']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Let's try the direct file path\n",
    "file_path = r\"C:\\Users\\noahi\\Downloads\\delivery_cost_comparison\"\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    print(\"üéØ File Found! Loading preview...\")\n",
    "    df = pd.read_csv(file_path, nrows=5)\n",
    "    print(\"\\n--- Columns ---\")\n",
    "    print(df.columns.tolist())\n",
    "    print(\"\\n--- Data Sample ---\")\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"‚ùå Path is still wrong. Let's check the Downloads folder for the real name:\")\n",
    "    downloads = r\"C:\\Users\\noahi\\Downloads\"\n",
    "    # This will list only FOLDERS in your downloads to find the right one\n",
    "    folders = [f for f in os.listdir(downloads) if os.path.isdir(os.path.join(downloads, f))]\n",
    "    print(f\"Folders found in Downloads: {folders}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9ed9553-ce69-406e-82fc-4fa70a0179c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 4th Dataset: Comparison Data ---\n",
      "Columns: ['delivery_id', 'timestamp', 'type', 'cost', 'time_taken', 'distance']\n",
      "\n",
      "--- Data Sample ---\n",
      "   delivery_id            timestamp   type       cost  time_taken  distance\n",
      "0          1.0  2024-11-06 18:32:00  human  16.300128   28.276063  2.809946\n",
      "1          2.0  2025-05-26 10:26:00  robot   6.189864   19.796571  0.694347\n",
      "2          3.0  2022-02-27 03:01:00  robot   2.452831   25.755434  4.849879\n",
      "3          4.0  2022-08-13 04:12:00  robot  17.155278   20.705736  2.121815\n",
      "4          5.0  2021-02-07 04:09:00  human  17.500188   22.654103  3.174608\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Using your exact verified path\n",
    "path = r\"C:\\Users\\noahi\\Downloads\\delivery_cost_comparison.csv\\delivery_cost_comparison.csv\"\n",
    "\n",
    "try:\n",
    "    df_preview = pd.read_csv(path, nrows=5)\n",
    "    print(\"--- 4th Dataset: Comparison Data ---\")\n",
    "    print(f\"Columns: {df_preview.columns.tolist()}\")\n",
    "    print(\"\\n--- Data Sample ---\")\n",
    "    print(df_preview.head())\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2cc5700-2a27-4564-b091-1403fd7dbbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Comparison Dataset Cleaned, Blanks Dropped, and Rounded!\n",
      "\n",
      "--- FINAL COST COMPARISON AUDIT ---\n",
      "              cost  time_taken  distance\n",
      "type                                    \n",
      "human    10.484397   17.493650  2.551748\n",
      "invalid  10.532591   17.163290  2.585946\n",
      "robot    10.497891   17.491233  2.548761\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Your verified double-extension path\n",
    "input_path = r\"C:\\Users\\noahi\\Downloads\\delivery_cost_comparison.csv\\delivery_cost_comparison.csv\"\n",
    "output_path = r\"C:\\Users\\noahi\\Downloads\\delivery_cost_comparison_PROFESSIONAL.csv\"\n",
    "\n",
    "# 1. Load Data\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# 2. DROP BLANKS (Removes any row that is completely empty or missing critical data)\n",
    "df = df.dropna(how='any') \n",
    "\n",
    "# 3. Basic Cleanup\n",
    "df = df.drop_duplicates(subset=['delivery_id'])\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "\n",
    "# FIX: Added .str before .strip() to avoid that AttributeError\n",
    "df['type'] = df['type'].str.lower().str.strip() \n",
    "\n",
    "# 4. Surgical Outlier Removal (The IQR Method)\n",
    "for col in ['cost', 'time_taken', 'distance']:\n",
    "    # Ensure column is numeric\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Find the \"normal\" range\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Replace \"999 million\" type placeholders with the median\n",
    "    median_val = df[col].median()\n",
    "    df.loc[(df[col] < lower_bound) | (df[col] > upper_bound), col] = median_val\n",
    "    \n",
    "    # Round for professional readability\n",
    "    df[col] = df[col].round(2)\n",
    "\n",
    "# 5. Save\n",
    "df.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ Comparison Dataset Cleaned, Blanks Dropped, and Rounded!\")\n",
    "\n",
    "# 6. The \"Big Reveal\": Robot vs Human\n",
    "print(\"\\n--- FINAL COST COMPARISON AUDIT ---\")\n",
    "print(df.groupby('type')[['cost', 'time_taken', 'distance']].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0b95874-bfaa-45a3-b682-418228fc6e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- REFINED AUDIT (MEDIANS) ---\n",
      "        cost  time_taken  distance\n",
      "type                              \n",
      "human  10.48       17.49      2.55\n",
      "robot  10.48       17.49      2.55\n",
      "\n",
      "--- DATA VOLUME ---\n",
      "type\n",
      "human    414321\n",
      "robot    412591\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load our newly cleaned professional file\n",
    "path = r\"C:\\Users\\noahi\\Downloads\\delivery_cost_comparison_PROFESSIONAL.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# 1. Remove the \"invalid\" rows\n",
    "df = df[df['type'] != 'invalid']\n",
    "\n",
    "# 2. Let's look at the MEDIAN instead of the Mean\n",
    "print(\"\\n--- REFINED AUDIT (MEDIANS) ---\")\n",
    "print(df.groupby('type')[['cost', 'time_taken', 'distance']].median())\n",
    "\n",
    "# 3. Let's see the Count - how many of each do we actually have?\n",
    "print(\"\\n--- DATA VOLUME ---\")\n",
    "print(df['type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c96fc5fd-5c92-495b-80bd-039ecb2a796a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version              | File Size \n",
      "-----------------------------------\n",
      "Original Raw         | 71.91 MB\n",
      "Professional Final   | 40.97 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "orig_path = r\"C:\\Users\\noahi\\Downloads\\delivery_cost_comparison.csv\\delivery_cost_comparison.csv\"\n",
    "final_path = r\"C:\\Users\\noahi\\Downloads\\delivery_cost_comparison_PROFESSIONAL.csv\"\n",
    "\n",
    "def get_mb(path):\n",
    "    return f\"{os.path.getsize(path) / (1024 * 1024):.2f} MB\"\n",
    "\n",
    "print(f\"{'Version':<20} | {'File Size':<10}\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"{'Original Raw':<20} | {get_mb(orig_path)}\")\n",
    "print(f\"{'Professional Final':<20} | {get_mb(final_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65b7e70c-50b8-4a81-84b2-f21165597ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 6th Dataset: Customer Satisfaction ---\n",
      "Columns: ['customer_id', 'timestamp', 'rating', 'retention_flag', 'feedback_length']\n",
      "\n",
      "--- Data Sample ---\n",
      "   customer_id            timestamp  rating  retention_flag  feedback_length\n",
      "0       8737.0  2020-11-09 16:45:00     4.0             1.0             99.0\n",
      "1       4989.0  2025-09-16 02:14:00     5.0             1.0            168.0\n",
      "2       2928.0  2024-07-15 09:22:00     5.0             1.0            170.0\n",
      "3       9810.0  2021-10-20 13:54:00     4.0             1.0            131.0\n",
      "4       1650.0  2020-05-03 02:53:00     1.0             1.0            197.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your newest file (adjusting for the typo just in case)\n",
    "path = r\"C:\\Users\\noahi\\Downloads\\customer_satisfaction_metrics.csv\\customer_satisfaction_metrics.csv\"\n",
    "\n",
    "try:\n",
    "    df_preview = pd.read_csv(path, nrows=5)\n",
    "    print(\"--- 6th Dataset: Customer Satisfaction ---\")\n",
    "    print(f\"Columns: {df_preview.columns.tolist()}\")\n",
    "    print(\"\\n--- Data Sample ---\")\n",
    "    print(df_preview)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}. Check if the file is named 'customer_satisfaction_metrics.csv' or 'cumstomer_satisfaction_metrics.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f4b3d2f-ae66-40ea-8709-4a4cde9384cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IntCastingNaNError",
     "evalue": "Cannot convert non-finite values (NA or inf) to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIntCastingNaNError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m median_len = df[\u001b[33m'\u001b[39m\u001b[33mfeedback_length\u001b[39m\u001b[33m'\u001b[39m].median()\n\u001b[32m     29\u001b[39m df.loc[df[\u001b[33m'\u001b[39m\u001b[33mfeedback_length\u001b[39m\u001b[33m'\u001b[39m] > upper_limit, \u001b[33m'\u001b[39m\u001b[33mfeedback_length\u001b[39m\u001b[33m'\u001b[39m] = median_len\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mfeedback_length\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfeedback_length\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mround\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# 6. Save\u001b[39;00m\n\u001b[32m     33\u001b[39m df.to_csv(output_path, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py:6643\u001b[39m, in \u001b[36mNDFrame.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m   6637\u001b[39m     results = [\n\u001b[32m   6638\u001b[39m         ser.astype(dtype, copy=copy, errors=errors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items()\n\u001b[32m   6639\u001b[39m     ]\n\u001b[32m   6641\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6642\u001b[39m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6643\u001b[39m     new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6644\u001b[39m     res = \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes)\n\u001b[32m   6645\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mastype\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:430\u001b[39m, in \u001b[36mBaseBlockManager.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[32m    428\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mastype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[43m=\u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:758\u001b[39m, in \u001b[36mBlock.astype\u001b[39m\u001b[34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[39m\n\u001b[32m    755\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan not squeeze with more than one column.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    756\u001b[39m     values = values[\u001b[32m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m new_values = \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m new_values = maybe_coerce_values(new_values)\n\u001b[32m    762\u001b[39m refs = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:237\u001b[39m, in \u001b[36mastype_array_safe\u001b[39m\u001b[34m(values, dtype, copy, errors)\u001b[39m\n\u001b[32m    234\u001b[39m     dtype = dtype.numpy_dtype\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     new_values = \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:182\u001b[39m, in \u001b[36mastype_array\u001b[39m\u001b[34m(values, dtype, copy)\u001b[39m\n\u001b[32m    179\u001b[39m     values = values.astype(dtype, copy=copy)\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     values = \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np.dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values.dtype.type, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:101\u001b[39m, in \u001b[36m_astype_nansafe\u001b[39m\u001b[34m(arr, dtype, copy, skipna)\u001b[39m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.ensure_string_array(\n\u001b[32m     97\u001b[39m         arr, skipna=skipna, convert_na_value=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     98\u001b[39m     ).reshape(shape)\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m np.issubdtype(arr.dtype, np.floating) \u001b[38;5;129;01mand\u001b[39;00m dtype.kind \u001b[38;5;129;01min\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33miu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_astype_float_to_int_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m arr.dtype == \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m    104\u001b[39m     \u001b[38;5;66;03m# if we have a datetime/timedelta array of objects\u001b[39;00m\n\u001b[32m    105\u001b[39m     \u001b[38;5;66;03m# then coerce to datetime64[ns] and use DatetimeArray.astype\u001b[39;00m\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m lib.is_np_dtype(dtype, \u001b[33m\"\u001b[39m\u001b[33mM\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:145\u001b[39m, in \u001b[36m_astype_float_to_int_nansafe\u001b[39m\u001b[34m(values, dtype, copy)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[33;03mastype with a check preventing converting NaN to an meaningless integer value.\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isfinite(values).all():\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m IntCastingNaNError(\n\u001b[32m    146\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot convert non-finite values (NA or inf) to integer\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    147\u001b[39m     )\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype.kind == \u001b[33m\"\u001b[39m\u001b[33mu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    149\u001b[39m     \u001b[38;5;66;03m# GH#45151\u001b[39;00m\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (values >= \u001b[32m0\u001b[39m).all():\n",
      "\u001b[31mIntCastingNaNError\u001b[39m: Cannot convert non-finite values (NA or inf) to integer"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Path to your file\n",
    "input_path = r\"C:\\Users\\noahi\\Downloads\\customer_satisfaction_metrics.csv\\customer_satisfaction_metrics.csv\"\n",
    "output_path = r\"C:\\Users\\noahi\\Downloads\\customer_satisfaction_PROFESSIONAL.csv\"\n",
    "\n",
    "# 1. Load Data\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# 2. Drop rows with missing Customer IDs or Ratings\n",
    "df = df.dropna(subset=['customer_id', 'rating'])\n",
    "\n",
    "# 3. Clean the Ratings (Force 1-5 scale)\n",
    "# If a rating is outside this, we'll cap it at the nearest bound\n",
    "df['rating'] = df['rating'].clip(lower=1, upper=5).round(0).astype(int)\n",
    "\n",
    "# 4. Clean Retention Flag (Ensure it's just 0 or 1)\n",
    "df['retention_flag'] = df['retention_flag'].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "\n",
    "# 5. Scrub Feedback Length Outliers (IQR Method)\n",
    "Q1 = df['feedback_length'].quantile(0.25)\n",
    "Q3 = df['feedback_length'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "upper_limit = Q3 + 1.5 * IQR\n",
    "\n",
    "# Replace reviews longer than the limit with the median length\n",
    "median_len = df['feedback_length'].median()\n",
    "df.loc[df['feedback_length'] > upper_limit, 'feedback_length'] = median_len\n",
    "df['feedback_length'] = df['feedback_length'].round(0).astype(int)\n",
    "\n",
    "# 6. Save\n",
    "df.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ Satisfaction Metrics Cleaned!\")\n",
    "\n",
    "# 7. Quick Professional Insight\n",
    "print(\"\\n--- SATISFACTION SUMMARY ---\")\n",
    "print(f\"Average Rating: {df['rating'].mean():.2f} / 5.0\")\n",
    "print(f\"Retention Rate: {df['retention_flag'].mean()*100:.1f}%\")\n",
    "print(\"\\n--- Ratings Distribution ---\")\n",
    "print(df['rating'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d56e291e-499b-4f2a-9c01-6349e43f6d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Satisfaction Metrics Cleaned and Blank Rows Removed!\n",
      "\n",
      "--- FINAL SATISFACTION AUDIT ---\n",
      "Total Responses: 823,588\n",
      "Average Rating: 3.00 / 5.0\n",
      "Retention Rate: -702055416933944448.0%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Path to your file\n",
    "input_path = r\"C:\\Users\\noahi\\Downloads\\customer_satisfaction_metrics.csv\\customer_satisfaction_metrics.csv\"\n",
    "output_path = r\"C:\\Users\\noahi\\Downloads\\customer_satisfaction_PROFESSIONAL.csv\"\n",
    "\n",
    "# 1. Load Data\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# 2. DROP ALL BLANKS AND INFINITIES (The Fix)\n",
    "# This removes rows with missing values in critical columns\n",
    "df = df.dropna(subset=['customer_id', 'rating', 'retention_flag', 'feedback_length'])\n",
    "\n",
    "# Also replace any hidden 'inf' with the median to prevent casting errors\n",
    "df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "# 3. Clean the Ratings (Force 1-5 scale)\n",
    "df['rating'] = df['rating'].clip(lower=1, upper=5).round(0).astype(int)\n",
    "\n",
    "# 4. Clean Retention Flag (Ensure it's just 0 or 1)\n",
    "df['retention_flag'] = df['retention_flag'].round(0).astype(int)\n",
    "\n",
    "# 5. Scrub Feedback Length Outliers\n",
    "Q1 = df['feedback_length'].quantile(0.25)\n",
    "Q3 = df['feedback_length'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "upper_limit = Q3 + 1.5 * IQR\n",
    "\n",
    "median_len = df['feedback_length'].median()\n",
    "df.loc[df['feedback_length'] > upper_limit, 'feedback_length'] = median_len\n",
    "df['feedback_length'] = df['feedback_length'].round(0).astype(int)\n",
    "\n",
    "# 6. Save\n",
    "df.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ Satisfaction Metrics Cleaned and Blank Rows Removed!\")\n",
    "\n",
    "# 7. Professional Insight Audit\n",
    "print(\"\\n--- FINAL SATISFACTION AUDIT ---\")\n",
    "print(f\"Total Responses: {len(df):,}\")\n",
    "print(f\"Average Rating: {df['rating'].mean():.2f} / 5.0\")\n",
    "print(f\"Retention Rate: {df['retention_flag'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a1b430c-96c5-46a9-8f08-2520f8708e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ NUCLEAR CLEAN COMPLETE!\n",
      "\n",
      "--- FINAL SATISFACTION AUDIT ---\n",
      "Total Valid Rows: 826,834\n",
      "Average Rating: 3.00 / 5.0\n",
      "Retention Rate: 89.98%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Path to your file\n",
    "input_path = r\"C:\\Users\\noahi\\Downloads\\customer_satisfaction_metrics.csv\\customer_satisfaction_metrics.csv\"\n",
    "output_path = r\"C:\\Users\\noahi\\Downloads\\customer_satisfaction_PROFESSIONAL.csv\"\n",
    "\n",
    "# 1. Load Data\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# 2. DROP DUPLICATES (Immediate cleanup)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# 3. CONVERT TO NUMERIC (The Fix)\n",
    "# This turns anything weird into \"NaN\" so we can drop it easily\n",
    "cols_to_fix = ['rating', 'retention_flag', 'feedback_length']\n",
    "for col in cols_to_fix:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# 4. DROP ALL BLANKS AND INFINITIES\n",
    "# This removes the rows that were causing your IntCastingNaNError\n",
    "df = df.dropna(subset=cols_to_fix)\n",
    "df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=cols_to_fix)\n",
    "\n",
    "# 5. HARD FILTER (The Ghost-Buster)\n",
    "# This removes the crazy numbers that broke your Retention Rate\n",
    "df = df[(df['rating'] >= 1) & (df['rating'] <= 5)]\n",
    "df = df[df['retention_flag'].isin([0, 1])]\n",
    "\n",
    "# 6. NOW cast to Integer (This will not fail now)\n",
    "df['rating'] = df['rating'].astype(int)\n",
    "df['retention_flag'] = df['retention_flag'].astype(int)\n",
    "df['feedback_length'] = df['feedback_length'].astype(int)\n",
    "\n",
    "# 7. Save\n",
    "df.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ NUCLEAR CLEAN COMPLETE!\")\n",
    "\n",
    "# 8. FINAL AUDIT\n",
    "print(\"\\n--- FINAL SATISFACTION AUDIT ---\")\n",
    "print(f\"Total Valid Rows: {len(df):,}\")\n",
    "print(f\"Average Rating: {df['rating'].mean():.2f} / 5.0\")\n",
    "print(f\"Retention Rate: {df['retention_flag'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "966036ed-778c-4236-8ce6-910edf067df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FILE SIZE & INTEGRITY AUDIT ---\n",
      "Metric               | Raw File        | Clean File     \n",
      "-------------------------------------------------------\n",
      "Size (MB)            |        32.08 MB |        27.81 MB\n",
      "Row Count            |         835,071 |         826,834\n",
      "Rows Removed         |               - |           8,237\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to your files\n",
    "raw_path = r\"C:\\Users\\noahi\\Downloads\\customer_satisfaction_metrics.csv\\customer_satisfaction_metrics.csv\"\n",
    "clean_path = r\"C:\\Users\\noahi\\Downloads\\customer_satisfaction_PROFESSIONAL.csv\"\n",
    "\n",
    "def get_stats(file_path):\n",
    "    # Get size in Megabytes\n",
    "    size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "    # Get row count\n",
    "    df = pd.read_csv(file_path)\n",
    "    rows = len(df)\n",
    "    return size_mb, rows\n",
    "\n",
    "# Execute Audit\n",
    "raw_size, raw_rows = get_stats(raw_path)\n",
    "clean_size, clean_rows = get_stats(clean_path)\n",
    "\n",
    "print(\"--- FILE SIZE & INTEGRITY AUDIT ---\")\n",
    "print(f\"{'Metric':<20} | {'Raw File':<15} | {'Clean File':<15}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Size (MB)':<20} | {raw_size:>12.2f} MB | {clean_size:>12.2f} MB\")\n",
    "print(f\"{'Row Count':<20} | {raw_rows:>15,} | {clean_rows:>15,}\")\n",
    "print(f\"{'Rows Removed':<20} | {'-':>15} | {raw_rows - clean_rows:>15,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd39a47e-d651-4cbe-972c-d870c4e25398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: Blanks removed, Duplicates dropped, and Outliers replaced with Medians!\n",
      "\n",
      "--- SIZE REPORT ---\n",
      "Raw File Size:   32.08 MB\n",
      "Clean File Size: 27.99 MB\n",
      "Final Row Count: 830,540\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "input_path = r\"C:\\Users\\noahi\\Downloads\\customer_satisfaction_metrics.csv\\customer_satisfaction_metrics.csv\"\n",
    "output_path = r\"C:\\Users\\noahi\\Downloads\\customer_satisfaction_PROFESSIONAL.csv\"\n",
    "\n",
    "# 1. Load and Remove Duplicates immediately\n",
    "df = pd.read_csv(input_path)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# 2. Drop Blank Rows for critical IDs\n",
    "# We drop if customer_id or timestamp is blank because we can't \"guess\" those\n",
    "df = df.dropna(subset=['customer_id', 'timestamp'])\n",
    "\n",
    "# 3. Handle Numerical Columns (Rating & Feedback Length)\n",
    "cols_to_fix = ['rating', 'feedback_length', 'retention_flag']\n",
    "\n",
    "for col in cols_to_fix:\n",
    "    # Convert to numeric, forcing errors to NaN\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Calculate the Median for this column\n",
    "    col_median = df[col].median()\n",
    "    \n",
    "    # Identify Outliers (IQR Method)\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # REPLACEMENT LOGIC:\n",
    "    # If it's a NaN OR an outlier, replace with the Median\n",
    "    df.loc[(df[col].isna()) | (df[col] < lower_bound) | (df[col] > upper_bound), col] = col_median\n",
    "\n",
    "# 4. Final Type Fixing (Clean whole numbers)\n",
    "df['rating'] = df['rating'].round(0).astype(int)\n",
    "df['retention_flag'] = df['retention_flag'].round(0).astype(int)\n",
    "df['feedback_length'] = df['feedback_length'].round(0).astype(int)\n",
    "\n",
    "# 5. Save and Audit\n",
    "df.to_csv(output_path, index=False)\n",
    "print(\"‚úÖ SUCCESS: Blanks removed, Duplicates dropped, and Outliers replaced with Medians!\")\n",
    "\n",
    "# Before/After Size Check\n",
    "import os\n",
    "raw_size = os.path.getsize(input_path) / (1024 * 1024)\n",
    "clean_size = os.path.getsize(output_path) / (1024 * 1024)\n",
    "\n",
    "print(f\"\\n--- SIZE REPORT ---\")\n",
    "print(f\"Raw File Size:   {raw_size:.2f} MB\")\n",
    "print(f\"Clean File Size: {clean_size:.2f} MB\")\n",
    "print(f\"Final Row Count: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd8a0bb5-ef4e-4d01-95db-1118fd849e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dataset: customer_behavior.csv ---\n",
      "Columns: ['customer_id', 'timestamp', 'action_type', 'item_id', 'session_duration', 'device_type']\n",
      "\n",
      "--- Data Sample ---\n",
      "   customer_id            timestamp action_type  item_id  session_duration  \\\n",
      "0        571.0  2023-04-08 21:00:00       order    210.0         23.146134   \n",
      "1        407.0  2022-10-20 03:38:00     abandon    176.0        250.761297   \n",
      "2       6270.0  2023-08-30 19:48:00        view    499.0        134.534415   \n",
      "3       9844.0  2023-01-10 17:17:00     abandon     72.0        214.301357   \n",
      "4       1597.0  2024-11-05 21:21:00     abandon    276.0        276.243561   \n",
      "\n",
      "  device_type  \n",
      "0         app  \n",
      "1      mobile  \n",
      "2     desktop  \n",
      "3     desktop  \n",
      "4         app  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Update this filename for whichever of the 5 you want to do next\n",
    "file_name = \"customer_behavior.csv\" \n",
    "path = rf\"C:\\Users\\noahi\\Downloads\\customer_behavior.csv\\customer_behavior.csv\"\n",
    "\n",
    "df_preview = pd.read_csv(path, nrows=5)\n",
    "print(f\"--- Dataset: {file_name} ---\")\n",
    "print(f\"Columns: {df_preview.columns.tolist()}\")\n",
    "print(\"\\n--- Data Sample ---\")\n",
    "print(df_preview.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35b87428-0be5-4456-8004-32bd3bc0c6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Rows: 737,904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noahi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1920: RuntimeWarning: overflow encountered in multiply\n",
      "  values = self.values.round(decimals)  # type: ignore[union-attr]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- CLEANING COMPLETE ---\n",
      "Final Row Count: 734,038\n",
      "Median Session Duration: 150.39 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Retrieve the data\n",
    "input_path = r\"C:\\Users\\noahi\\Downloads\\customer_behavior.csv\\customer_behavior.csv\"\n",
    "output_path = r\"C:\\Users\\noahi\\Downloads\\customer_behavior_PROFESSIONAL.csv\"\n",
    "\n",
    "df = pd.read_csv(input_path)\n",
    "print(f\"Initial Rows: {len(df):,}\")\n",
    "\n",
    "# 2. Drop Duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# 3. Remove Blanks\n",
    "# We drop rows if essential IDs (who, what, when) are missing\n",
    "df = df.dropna(subset=['customer_id', 'timestamp', 'action_type'])\n",
    "\n",
    "# 4. Replace Outliers with Median (for 'session_duration')\n",
    "# First, ensure it's a number\n",
    "df['session_duration'] = pd.to_numeric(df['session_duration'], errors='coerce')\n",
    "\n",
    "# Fill actual NaN values with median before outlier detection\n",
    "duration_median = df['session_duration'].median()\n",
    "df['session_duration'] = df['session_duration'].fillna(duration_median)\n",
    "\n",
    "# Use IQR to find \"Glitch\" durations (like 999999 seconds)\n",
    "Q1 = df['session_duration'].quantile(0.25)\n",
    "Q3 = df['session_duration'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "upper_limit = Q3 + 1.5 * IQR\n",
    "\n",
    "# Replace any duration above the limit with the median\n",
    "df.loc[df['session_duration'] > upper_limit, 'session_duration'] = duration_median\n",
    "\n",
    "# 5. Final Formatting\n",
    "df['session_duration'] = df['session_duration'].round(1) # Keeping one decimal for seconds\n",
    "\n",
    "# Save the professional version\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"\\n--- CLEANING COMPLETE ---\")\n",
    "print(f\"Final Row Count: {len(df):,}\")\n",
    "print(f\"Median Session Duration: {duration_median:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7733286-21ee-4aa8-a94a-712e2bdccce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dataset: delivery_reliability.csv ---\n",
      "Columns: ['customer_id', 'timestamp', 'action_type', 'item_id', 'session_duration', 'device_type']\n",
      "\n",
      "--- Data Sample ---\n",
      "   customer_id            timestamp action_type  item_id  session_duration  \\\n",
      "0        571.0  2023-04-08 21:00:00       order    210.0         23.146134   \n",
      "1        407.0  2022-10-20 03:38:00     abandon    176.0        250.761297   \n",
      "2       6270.0  2023-08-30 19:48:00        view    499.0        134.534415   \n",
      "3       9844.0  2023-01-10 17:17:00     abandon     72.0        214.301357   \n",
      "4       1597.0  2024-11-05 21:21:00     abandon    276.0        276.243561   \n",
      "\n",
      "  device_type  \n",
      "0         app  \n",
      "1      mobile  \n",
      "2     desktop  \n",
      "3     desktop  \n",
      "4         app  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the reliability dataset\n",
    "path = r\"C:\\Users\\noahi\\Downloads\\customer_behavior.csv\\customer_behavior.csv\"\n",
    "\n",
    "df_preview = pd.read_csv(path, nrows=5)\n",
    "print(f\"--- Dataset: delivery_reliability.csv ---\")\n",
    "print(f\"Columns: {df_preview.columns.tolist()}\")\n",
    "print(\"\\n--- Data Sample ---\")\n",
    "print(df_preview.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0048621d-2b3b-46aa-ab95-dd557e88e4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error: ['delivery_id', 'status']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Retrieve the data\n",
    "input_path = r\"C:\\Users\\noahi\\Downloads\\customer_behavior.csv\\customer_behavior.csv\"\n",
    "output_path = r\"C:\\Users\\noahi\\Downloads\\delivery_reliability_PROFESSIONAL.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(input_path)\n",
    "    initial_count = len(df)\n",
    "\n",
    "    # 2. Step: Remove Duplicates\n",
    "    df = df.drop_duplicates()\n",
    "    after_dup_count = len(df)\n",
    "\n",
    "    # 3. Step: Remove Blank Rows\n",
    "    # We drop rows where delivery_id or status is missing\n",
    "    df = df.dropna(subset=['delivery_id', 'status'])\n",
    "\n",
    "    # 4. Step: Replace Outliers with Median\n",
    "    # Identifying numeric columns automatically\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        col_median = df[col].median()\n",
    "        \n",
    "        # IQR Outlier Detection\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_limit = Q1 - 1.5 * IQR\n",
    "        upper_limit = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Swap outliers/NaNs with the median\n",
    "        df.loc[(df[col] < lower_limit) | (df[col] > upper_limit) | (df[col].isna()), col] = col_median\n",
    "\n",
    "    # 5. Save the Professional version\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(\"--- RELIABILITY CLEANING REPORT ---\")\n",
    "    print(f\"Initial Rows:     {initial_count:,}\")\n",
    "    print(f\"Duplicates Removed: {initial_count - after_dup_count:,}\")\n",
    "    print(f\"Final Clean Rows:   {len(df):,}\")\n",
    "    print(f\"Numerical Columns Fixed: {numeric_cols}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73480b95-8a82-45d1-9a53-715a7e5e1259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Columns: ['customer_id', 'timestamp', 'action_type', 'item_id', 'session_duration', 'device_type']\n",
      "\n",
      "‚úÖ CLEANING SUCCESSFUL!\n",
      "Final Row Count: 736,940\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Retrieve the data\n",
    "input_path = r\"C:\\Users\\noahi\\Downloads\\customer_behavior.csv\\customer_behavior.csv\"\n",
    "output_path = r\"C:\\Users\\noahi\\Downloads\\delivery_reliability_PROFESSIONAL.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(input_path)\n",
    "    print(f\"Detected Columns: {df.columns.tolist()}\")\n",
    "\n",
    "    # 2. REMOVE DUPLICATES (The Step you requested)\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # 3. REMOVE BLANKS\n",
    "    # Instead of guessing names, we drop rows that are completely empty\n",
    "    # and then drop rows where the first column (usually the ID) is missing.\n",
    "    df = df.dropna(how='all')\n",
    "    df = df.dropna(subset=[df.columns[0]])\n",
    "\n",
    "    # 4. REPLACE OUTLIERS WITH MEDIAN\n",
    "    # This automatically finds every column with numbers\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        col_median = df[col].median()\n",
    "        \n",
    "        # Identify outliers using IQR\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Replace anything outside the lines or NaN with the median\n",
    "        df.loc[(df[col] < lower) | (df[col] > upper) | (df[col].isna()), col] = col_median\n",
    "\n",
    "    # 5. Save\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(\"\\n‚úÖ CLEANING SUCCESSFUL!\")\n",
    "    print(f\"Final Row Count: {len(df):,}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Still getting an error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7af059a-23a6-4d8d-8b13-9fb41cca8993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RELIABILITY SIZE AUDIT ---\n",
      "Metric               | Original        | Cleaned        \n",
      "-------------------------------------------------------\n",
      "Size (MB)            |        44.79 MB |        45.38 MB\n",
      "Total Rows           |         737,905 |         736,941\n",
      "Reduction (%)        |               - |           -1.3%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Paths to your specific files\n",
    "raw_file = r\"C:\\Users\\noahi\\Downloads\\customer_behavior.csv\\customer_behavior.csv\"\n",
    "clean_file = r\"C:\\Users\\noahi\\Downloads\\delivery_reliability_PROFESSIONAL.csv\"\n",
    "\n",
    "def get_stats(path):\n",
    "    size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "    # Quick count of lines without loading the whole thing into memory\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        row_count = sum(1 for line in f)\n",
    "    return size_mb, row_count\n",
    "\n",
    "# Calculate\n",
    "raw_size, raw_rows = get_stats(raw_file)\n",
    "clean_size, clean_rows = get_stats(clean_file)\n",
    "\n",
    "print(\"--- RELIABILITY SIZE AUDIT ---\")\n",
    "print(f\"{'Metric':<20} | {'Original':<15} | {'Cleaned':<15}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Size (MB)':<20} | {raw_size:>12.2f} MB | {clean_size:>12.2f} MB\")\n",
    "print(f\"{'Total Rows':<20} | {raw_rows:>15,} | {clean_rows:>15,}\")\n",
    "print(f\"{'Reduction (%)':<20} | {'-':>15} | {((raw_size-clean_size)/raw_size)*100:>14.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bec1ec-7a9d-4954-ad38-1c863d83abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Paths to your specific files\n",
    "raw_file = r\"C:\\Users\\noahi\\Downloads\\customer_behavior.csv\\customer_behavior.csv\"\n",
    "clean_file = r\"C:\\Users\\noahi\\Downloads\\delivery_reliability_PROFESSIONAL.csv\"\n",
    "\n",
    "def get_stats(path):\n",
    "    size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "    # Quick count of lines without loading the whole thing into memory\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        row_count = sum(1 for line in f)\n",
    "    return size_mb, row_count\n",
    "\n",
    "# Calculate\n",
    "raw_size, raw_rows = get_stats(raw_file)\n",
    "clean_size, clean_rows = get_stats(clean_file)\n",
    "\n",
    "print(\"--- RELIABILITY SIZE AUDIT ---\")\n",
    "print(f\"{'Metric':<20} | {'Original':<15} | {'Cleaned':<15}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Size (MB)':<20} | {raw_size:>12.2f} MB | {clean_size:>12.2f} MB\")\n",
    "print(f\"{'Total Rows':<20} | {raw_rows:>15,} | {clean_rows:>15,}\")\n",
    "print(f\"{'Reduction (%)':<20} | {'-':>15} | {((raw_size-clean_size)/raw_size)*100:>14.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "561bbb38-fcd2-4410-b6cd-637efbfa6bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CUSTOMER BEHAVIOR: SURGICAL AUDIT ---\n",
      "Metric                    | Original        | Cleaned        \n",
      "------------------------------------------------------------\n",
      "File Size (MB)            |        44.79 MB |        45.24 MB\n",
      "Total Row Count           |         737,904 |         734,038\n",
      "Median Duration           |               - |       150.39 s\n",
      "Rows Removed              |               - |           3,866\n",
      "Efficiency Gain           |               - |           -1.0%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 1. Paths\n",
    "input_path = r\"C:\\Users\\noahi\\Downloads\\customer_behavior.csv\\customer_behavior.csv\"\n",
    "output_path = r\"C:\\Users\\noahi\\Downloads\\customer_behavior_PROFESSIONAL.csv\"\n",
    "\n",
    "# 2. Retrieve & Initial Stats\n",
    "df = pd.read_csv(input_path)\n",
    "raw_size = os.path.getsize(input_path) / (1024 * 1024)\n",
    "raw_rows = len(df)\n",
    "\n",
    "# 3. Clean Duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# 4. Remove Blanks\n",
    "# Dropping rows where essential tracking info is missing\n",
    "df = df.dropna(subset=['customer_id', 'timestamp', 'action_type'])\n",
    "\n",
    "# 5. Median Replacement for Outliers (session_duration)\n",
    "df['session_duration'] = pd.to_numeric(df['session_duration'], errors='coerce')\n",
    "duration_median = df['session_duration'].median()\n",
    "\n",
    "# Use IQR to define \"Sane\" session limits\n",
    "Q1 = df['session_duration'].quantile(0.25)\n",
    "Q3 = df['session_duration'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "upper_limit = Q3 + 1.5 * IQR\n",
    "\n",
    "# Surgical fix: Replace NaNs and Outliers with Median\n",
    "df.loc[(df['session_duration'].isna()) | (df['session_duration'] > upper_limit), 'session_duration'] = duration_median\n",
    "\n",
    "# 6. Save Professional Version\n",
    "df.to_csv(output_path, index=False)\n",
    "clean_size = os.path.getsize(output_path) / (1024 * 1024)\n",
    "clean_rows = len(df)\n",
    "\n",
    "# 7. FINAL AUDIT REPORT\n",
    "print(\"--- CUSTOMER BEHAVIOR: SURGICAL AUDIT ---\")\n",
    "print(f\"{'Metric':<25} | {'Original':<15} | {'Cleaned':<15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'File Size (MB)':<25} | {raw_size:>12.2f} MB | {clean_size:>12.2f} MB\")\n",
    "print(f\"{'Total Row Count':<25} | {raw_rows:>15,} | {clean_rows:>15,}\")\n",
    "print(f\"{'Median Duration':<25} | {'-':>15} | {duration_median:>12.2f} s\")\n",
    "print(f\"{'Rows Removed':<25} | {'-':>15} | {raw_rows - clean_rows:>15,}\")\n",
    "print(f\"{'Efficiency Gain':<25} | {'-':>15} | {((raw_size - clean_size)/raw_size)*100:>14.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16443d95-1cc1-406c-b692-d700c0cb449c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dataset: customer_behavior_PROFESSIONAL.csv ---\n",
      "Columns: ['customer_id', 'timestamp', 'action_type', 'item_id', 'session_duration', 'device_type']\n",
      "\n",
      "--- Data Sample ---\n",
      "   customer_id            timestamp action_type  item_id  session_duration  \\\n",
      "0        571.0  2023-04-08 21:00:00       order    210.0         23.146134   \n",
      "1        407.0  2022-10-20 03:38:00     abandon    176.0        250.761297   \n",
      "2       6270.0  2023-08-30 19:48:00        view    499.0        134.534415   \n",
      "3       9844.0  2023-01-10 17:17:00     abandon     72.0        214.301357   \n",
      "4       1597.0  2024-11-05 21:21:00     abandon    276.0        276.243561   \n",
      "\n",
      "  device_type  \n",
      "0         app  \n",
      "1      mobile  \n",
      "2     desktop  \n",
      "3     desktop  \n",
      "4         app  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r\"C:\\Users\\noahi\\Downloads\\customer_behavior_PROFESSIONAL.csv\"\n",
    "df_preview = pd.read_csv(path, nrows=5)\n",
    "\n",
    "print(f\"--- Dataset: customer_behavior_PROFESSIONAL.csv ---\")\n",
    "print(f\"Columns: {df_preview.columns.tolist()}\")\n",
    "print(\"\\n--- Data Sample ---\")\n",
    "print(df_preview.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34829ba4-fa9e-4228-b815-f6c391b0cd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dataset: predictive_maintenance.csv ---\n",
      "Columns: ['robot_id', 'timestamp', 'battery_level', 'motor_temp', 'distance_traveled', 'error_codes', 'maintenance_flag']\n",
      "\n",
      "--- Data Sample ---\n",
      "   robot_id            timestamp  battery_level  motor_temp  \\\n",
      "0      18.0  2023-12-05 03:24:00      70.992485   39.208862   \n",
      "1      80.0  2021-09-22 12:10:00      29.772286   62.367604   \n",
      "2      17.0  2023-07-03 03:56:00      62.718273   37.202737   \n",
      "3      17.0  2024-11-13 11:11:00      89.210984   54.570332   \n",
      "4      18.0  2022-09-26 00:40:00      98.077244   61.612533   \n",
      "\n",
      "   distance_traveled error_codes  maintenance_flag  \n",
      "0           3.706236        none               0.0  \n",
      "1           8.277852        none               0.0  \n",
      "2           4.668508        E001               0.0  \n",
      "3           0.590138        none               0.0  \n",
      "4           7.707339        none               0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r\"C:\\Users\\noahi\\Downloads\\predictive_maintenance.csv\\predictive_maintenance.csv\"\n",
    "df_preview = pd.read_csv(path, nrows=5)\n",
    "\n",
    "print(f\"--- Dataset: predictive_maintenance.csv ---\")\n",
    "print(f\"Columns: {df_preview.columns.tolist()}\")\n",
    "print(\"\\n--- Data Sample ---\")\n",
    "print(df_preview.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4548e78f-8f99-49e6-82fc-caa9bb111627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PREDICTIVE MAINTENANCE: SURGICAL AUDIT ---\n",
      "Original Rows:    737,904\n",
      "Cleaned Rows:     736,296\n",
      "Rows Removed:     1,608\n",
      "Final Size:       62.64 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 1. Paths\n",
    "input_path = r\"C:\\Users\\noahi\\Downloads\\predictive_maintenance.csv\\predictive_maintenance.csv\"\n",
    "output_path = r\"C:\\Users\\noahi\\Downloads\\predictive_maintenance_PROFESSIONAL.csv\"\n",
    "\n",
    "# 2. Retrieve & Initial Stats\n",
    "df = pd.read_csv(input_path)\n",
    "raw_size = os.path.getsize(input_path) / (1024 * 1024)\n",
    "raw_rows = len(df)\n",
    "\n",
    "# 3. Clean Duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# 4. Remove Blanks\n",
    "# We drop rows if the Machine ID or the Failure Label is missing\n",
    "df = df.dropna(subset=[df.columns[0], df.columns[-1]])\n",
    "\n",
    "# 5. Median Replacement for Outliers\n",
    "# Identifying all sensor data (numeric columns)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "for col in numeric_cols:\n",
    "    col_median = df[col].median()\n",
    "    \n",
    "    # IQR Method to find sensor glitches\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_limit = Q1 - 1.5 * IQR\n",
    "    upper_limit = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Swap outliers with median\n",
    "    df.loc[(df[col] < lower_limit) | (df[col] > upper_limit) | (df[col].isna()), col] = col_median\n",
    "\n",
    "# 6. Save Professional Version\n",
    "df.to_csv(output_path, index=False)\n",
    "clean_size = os.path.getsize(output_path) / (1024 * 1024)\n",
    "clean_rows = len(df)\n",
    "\n",
    "# 7. FINAL MAINTENANCE AUDIT\n",
    "print(\"\\n--- PREDICTIVE MAINTENANCE: SURGICAL AUDIT ---\")\n",
    "print(f\"Original Rows:    {raw_rows:,}\")\n",
    "print(f\"Cleaned Rows:     {clean_rows:,}\")\n",
    "print(f\"Rows Removed:     {raw_rows - clean_rows:,}\")\n",
    "print(f\"Final Size:       {clean_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "520bdc38-6e2e-4c37-96d4-6d323ed0c3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DATA SIZE COMPARISON ---\n",
      "Original File: 62.26 MB\n",
      "Cleaned File:  62.64 MB\n",
      "Difference:    +0.38 MB\n",
      "Growth/Shrink: 0.6%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the file paths\n",
    "original_path = r\"C:\\Users\\noahi\\Downloads\\predictive_maintenance.csv\\predictive_maintenance.csv\"\n",
    "cleaned_path = r\"C:\\Users\\noahi\\Downloads\\predictive_maintenance_PROFESSIONAL.csv\"\n",
    "\n",
    "def get_file_size_mb(path):\n",
    "    \"\"\"Returns file size in Megabytes.\"\"\"\n",
    "    return os.path.getsize(path) / (1024 * 1024)\n",
    "\n",
    "# Calculate sizes\n",
    "size_raw = get_file_size_mb(original_path)\n",
    "size_clean = get_file_size_mb(cleaned_path)\n",
    "\n",
    "# Print the comparison\n",
    "print(\"--- DATA SIZE COMPARISON ---\")\n",
    "print(f\"Original File: {size_raw:.2f} MB\")\n",
    "print(f\"Cleaned File:  {size_clean:.2f} MB\")\n",
    "print(f\"Difference:    {size_clean - size_raw:+.2f} MB\")\n",
    "print(f\"Growth/Shrink: {((size_clean - size_raw) / size_raw) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14a04676-38e7-42e8-8d68-cdff8e7052a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FILE SEARCH RESULTS ---\n",
      "1. personalized_orders.csv\n",
      "2. personalized_orders.csv.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Looking for any file with 'order' in the name\n",
    "download_path = r\"C:\\Users\\noahi\\Downloads\"\n",
    "all_files = os.listdir(download_path)\n",
    "matching_files = [f for f in all_files if 'order' in f.lower()]\n",
    "\n",
    "print(\"--- FILE SEARCH RESULTS ---\")\n",
    "if matching_files:\n",
    "    for i, file in enumerate(matching_files):\n",
    "        print(f\"{i+1}. {file}\")\n",
    "else:\n",
    "    print(\"No files containing 'order' were found. Please check the folder!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c09f695b-854c-46e9-8bae-d6922229b10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File loaded successfully!\n",
      "--- Columns Found ---\n",
      "['customer_id', 'order_id', 'timestamp', 'item_id', 'quantity', 'category', 'price']\n",
      "\n",
      "--- Data Sample ---\n",
      "   customer_id  order_id            timestamp  item_id  quantity category  \\\n",
      "0       7362.0       1.0  2024-01-11 19:14:00    296.0       3.0     food   \n",
      "1       7612.0       2.0  2020-04-11 05:45:00     56.0       4.0    snack   \n",
      "2        393.0       3.0  2020-02-05 22:59:00     74.0       3.0    snack   \n",
      "\n",
      "       price  \n",
      "0   9.091472  \n",
      "1  11.098171  \n",
      "2   5.881802  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Use the 'r' prefix to avoid that Unicode error\n",
    "file_path = r\"C:\\Users\\noahi\\Downloads\\personalized_orders.csv\\personalized_orders.csv\"\n",
    "\n",
    "try:\n",
    "    # Load only the header to peek at the columns\n",
    "    df_preview = pd.read_csv(file_path, nrows=5)\n",
    "    print(\"‚úÖ File loaded successfully!\")\n",
    "    print(f\"--- Columns Found ---\")\n",
    "    print(df_preview.columns.tolist())\n",
    "    print(\"\\n--- Data Sample ---\")\n",
    "    print(df_preview.head(3))\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Still having trouble: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1da6873f-74b5-4d5c-8fc0-5f0b1d49d391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PERSONALIZED ORDERS: CLEANING AUDIT ---\n",
      "Original Rows:    737,904\n",
      "Cleaned Rows:     734,166\n",
      "Glitches Removed: 3,738\n",
      "Status:           PROFESSIONAL GRADE ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 1. Setup Paths\n",
    "input_path = r\"C:\\Users\\noahi\\Downloads\\personalized_orders.csv\\personalized_orders.csv\"\n",
    "output_path = r\"C:\\Users\\noahi\\Downloads\\personalized_orders_PROFESSIONAL.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(input_path)\n",
    "    raw_rows = len(df)\n",
    "\n",
    "    # 2. DEDUPLICATION\n",
    "    # We use order_id as the unique key to remove repeat entries\n",
    "    df = df.drop_duplicates(subset=['order_id'])\n",
    "\n",
    "    # 3. REMOVE BLANKS\n",
    "    # Drop rows missing crucial ID or price info\n",
    "    df = df.dropna(subset=['order_id', 'customer_id', 'price'])\n",
    "\n",
    "    # 4. MEDIAN OUTLIER REPLACEMENT\n",
    "    # Target columns: quantity and price\n",
    "    cols_to_fix = ['quantity', 'price']\n",
    "    \n",
    "    for col in cols_to_fix:\n",
    "        col_median = df[col].median()\n",
    "        \n",
    "        # Calculate IQR (Interquartile Range)\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Replace outliers with the median to keep the data realistic\n",
    "        df.loc[(df[col] < lower_bound) | (df[col] > upper_bound), col] = col_median\n",
    "\n",
    "    # 5. FINAL EXPORT\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(\"--- PERSONALIZED ORDERS: CLEANING AUDIT ---\")\n",
    "    print(f\"Original Rows:    {raw_rows:,}\")\n",
    "    print(f\"Cleaned Rows:     {len(df):,}\")\n",
    "    print(f\"Glitches Removed: {raw_rows - len(df):,}\")\n",
    "    print(f\"Status:           PROFESSIONAL GRADE ‚úÖ\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during cleanup: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d368f3eb-4cad-4da2-8a33-25bb92f05820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DATA SIZE COMPARISON ---\n",
      "Original File: 49.36 MB\n",
      "Cleaned File:  49.77 MB\n",
      "Difference:    +0.41 MB\n",
      "Growth/Shrink: 0.8%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the file paths\n",
    "original_path = r\"C:\\Users\\noahi\\Downloads\\personalized_orders.csv\\personalized_orders.csv\"\n",
    "cleaned_path = r\"C:\\Users\\noahi\\Downloads\\personalized_orders_PROFESSIONAL.csv\"\n",
    "\n",
    "def get_file_size_mb(path):\n",
    "    \"\"\"Returns file size in Megabytes.\"\"\"\n",
    "    return os.path.getsize(path) / (1024 * 1024)\n",
    "\n",
    "# Calculate sizes\n",
    "size_raw = get_file_size_mb(original_path)\n",
    "size_clean = get_file_size_mb(cleaned_path)\n",
    "\n",
    "# Print the comparison\n",
    "print(\"--- DATA SIZE COMPARISON ---\")\n",
    "print(f\"Original File: {size_raw:.2f} MB\")\n",
    "print(f\"Cleaned File:  {size_clean:.2f} MB\")\n",
    "print(f\"Difference:    {size_clean - size_raw:+.2f} MB\")\n",
    "print(f\"Growth/Shrink: {((size_clean - size_raw) / size_raw) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc55efff-65b2-454d-baaa-ad09aedf0885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
